{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"do/","text":"Do \u00b6","title":"Do"},{"location":"do/#do","text":"","title":"Do"},{"location":"do/End-2-End-Testing/","text":"End To End Testing for any Software Module \u00b6 I have provided step by step instructions for the end to end testing for software (Turbonomic) module which can be replicated for any software such as CP4I, CP4D etc. Turbonomic Repo - https://github.com/IBM/automation-turbonomic Follow the steps to implement the end-to-end testing \u00b6 (1) Checkout the Git repo from the https://github.com/cloud-native-toolkit/automation-solutions git clone https://github.com/cloud-native-toolkit/automation-solutions.git Output from above Git Clone CLI Cloning into 'automation-solutions'... remote: Enumerating objects: 3778, done. remote: Counting objects: 100% (1114/1114), done. remote: Compressing objects: 100% (326/326), done. remote: Total 3778 (delta 722), reused 1070 (delta 700), pack-reused 2664 Receiving objects: 100% (3778/3778), 10.72 MiB | 10.41 MiB/s, done. Resolving deltas: 100% (2441/2441), done (2) Clone the Git repo for the software (such as turbonomic) which needs to be end-to-end testing git clone https://github.com/IBM/automation-turbonomic.git Note: Make sure the you keep the automation solutions and automation-turbonomic in the same level directory since we will be generating the files that will go directly to automation-turbonomic folder. Otherwise, you need to copy the files and manually move to automation-turbonomic folder. (3) Navigate to the software folder named \"Turbonomic\" in my case and launch the script \"generate.sh\" .\\generate.sh Output from the above CLI Loading catalog from url: https://modules.cloudnativetoolkit.dev/index.yaml Name: 200-openshift-gitops Writing output to: ../../../../automation-turbonomic Loading catalog from url: https://modules.cloudnativetoolkit.dev/index.yaml Name: 250-turbonomic-multicloud Writing output to: ../../../../automation-turbonomic Copying Files Copying Configuration Note: Every software layer which requires common layer such as Gitops, Storage as well as configuration will have symbolic to the Shared location. (4) Navigate to the software (automation-turbonomic) and verify the files are generated as well as .github folder exist which is requires for the end-to-end test to run. (5) Add the end to end test logic in the verify-workflow.yaml (automation-turbonomic.github\\workflows) of the Software module to be tested Below example strategy with do the end-to-end testing for the Turbonomic software on IBM Cloud infrastructure with the storage ODF and Portworx. Strategy: matrix: flavor: - ibm storage: - odf - portworx (6) Add environment variables needed for this module in the verify-pr.yaml env: Home: IBMCloud_API_Key (7) Steps represents a sequence of tasks that will be executed as part of job Add the steps which needs to be executed in the sequence (7) Modify the 200-openshift-gitops BOM to support Gitea Make sure generated main.tf is referencing the Gitea variables inside Gitops Module in the main.tf module \"gitops_repo\" { source = \"github.com/cloud-native-toolkit/terraform-tools-gitops?ref=v1.21.0\" branch = var.gitops_repo_branch debug = var.debug gitea_host = module.gitea.host gitea_org = module.gitea.org gitea_token = module.gitea.token gitea_username = module.gitea.username \u2014\u2014 } (8) Copy the .mocks folder which has the configuration for BOM layer dependency. - If you have any specific dependency between layers, you can describe in the terragrunt.hcl Note: You can also validate the dependency if its configured properly by launching the container (.launch.sh) and run the CLI terragrunt graph-dependencies which displays the dependency graph (9) Trigger the module build which will kick off the end-to-end test for the software to be tested. - Watch the Github Actions TAB","title":"End-To-End Testing"},{"location":"do/End-2-End-Testing/#end-to-end-testing-for-any-software-module","text":"I have provided step by step instructions for the end to end testing for software (Turbonomic) module which can be replicated for any software such as CP4I, CP4D etc. Turbonomic Repo - https://github.com/IBM/automation-turbonomic","title":"End To End Testing for any Software Module"},{"location":"do/End-2-End-Testing/#follow-the-steps-to-implement-the-end-to-end-testing","text":"(1) Checkout the Git repo from the https://github.com/cloud-native-toolkit/automation-solutions git clone https://github.com/cloud-native-toolkit/automation-solutions.git Output from above Git Clone CLI Cloning into 'automation-solutions'... remote: Enumerating objects: 3778, done. remote: Counting objects: 100% (1114/1114), done. remote: Compressing objects: 100% (326/326), done. remote: Total 3778 (delta 722), reused 1070 (delta 700), pack-reused 2664 Receiving objects: 100% (3778/3778), 10.72 MiB | 10.41 MiB/s, done. Resolving deltas: 100% (2441/2441), done (2) Clone the Git repo for the software (such as turbonomic) which needs to be end-to-end testing git clone https://github.com/IBM/automation-turbonomic.git Note: Make sure the you keep the automation solutions and automation-turbonomic in the same level directory since we will be generating the files that will go directly to automation-turbonomic folder. Otherwise, you need to copy the files and manually move to automation-turbonomic folder. (3) Navigate to the software folder named \"Turbonomic\" in my case and launch the script \"generate.sh\" .\\generate.sh Output from the above CLI Loading catalog from url: https://modules.cloudnativetoolkit.dev/index.yaml Name: 200-openshift-gitops Writing output to: ../../../../automation-turbonomic Loading catalog from url: https://modules.cloudnativetoolkit.dev/index.yaml Name: 250-turbonomic-multicloud Writing output to: ../../../../automation-turbonomic Copying Files Copying Configuration Note: Every software layer which requires common layer such as Gitops, Storage as well as configuration will have symbolic to the Shared location. (4) Navigate to the software (automation-turbonomic) and verify the files are generated as well as .github folder exist which is requires for the end-to-end test to run. (5) Add the end to end test logic in the verify-workflow.yaml (automation-turbonomic.github\\workflows) of the Software module to be tested Below example strategy with do the end-to-end testing for the Turbonomic software on IBM Cloud infrastructure with the storage ODF and Portworx. Strategy: matrix: flavor: - ibm storage: - odf - portworx (6) Add environment variables needed for this module in the verify-pr.yaml env: Home: IBMCloud_API_Key (7) Steps represents a sequence of tasks that will be executed as part of job Add the steps which needs to be executed in the sequence (7) Modify the 200-openshift-gitops BOM to support Gitea Make sure generated main.tf is referencing the Gitea variables inside Gitops Module in the main.tf module \"gitops_repo\" { source = \"github.com/cloud-native-toolkit/terraform-tools-gitops?ref=v1.21.0\" branch = var.gitops_repo_branch debug = var.debug gitea_host = module.gitea.host gitea_org = module.gitea.org gitea_token = module.gitea.token gitea_username = module.gitea.username \u2014\u2014 } (8) Copy the .mocks folder which has the configuration for BOM layer dependency. - If you have any specific dependency between layers, you can describe in the terragrunt.hcl Note: You can also validate the dependency if its configured properly by launching the container (.launch.sh) and run the CLI terragrunt graph-dependencies which displays the dependency graph (9) Trigger the module build which will kick off the end-to-end test for the software to be tested. - Watch the Github Actions TAB","title":"Follow the steps to implement the end-to-end testing"},{"location":"getting-started/","text":"Getting started \u00b6 The getting started section provides you three initial getting started with IasCable and Software Everywhere . Lab 1: Getting started with the basics Deploy a Virtual Private Cloud in IBM Cloud with IasCable and initial Terraform modules from Software Everywhere . Lab 2: Use IasCable to create a VPC and a Red Hat OpenShift cluster on IBM Cloud Lab3: Use Software Everywhere and IasCable to setup GitOps on a Red Hat OpenShift Cluster in a Virtual Private Cloud on IBM Cloud","title":"Overview"},{"location":"getting-started/#getting-started","text":"The getting started section provides you three initial getting started with IasCable and Software Everywhere . Lab 1: Getting started with the basics Deploy a Virtual Private Cloud in IBM Cloud with IasCable and initial Terraform modules from Software Everywhere . Lab 2: Use IasCable to create a VPC and a Red Hat OpenShift cluster on IBM Cloud Lab3: Use Software Everywhere and IasCable to setup GitOps on a Red Hat OpenShift Cluster in a Virtual Private Cloud on IBM Cloud","title":"Getting started"},{"location":"getting-started/lab1/","text":"Lab 1: Getting start with the basics \u00b6 That lab project uses following resource as input: From IasCable and that blog post . This is just a starting point to use IasCable . 1. The IasCable framework \u00b6 Let us have first a look the basic components of the IasCable framework. Bill of Material and Modules \u00b6 The IasCable uses a Bill of Material and Modules (from the Software Everywhere project ), which you need to understand. These two parts are the heart of the framework we could say to realize the objective to build an installable component infrastructure based on components from a catalog of available modules. Please visit the linked resources for more details. Simplified we can say a by modules it uses, variables you can define and providers you can define. It is good to know that modules can depend on other modules, if this is the case the related modules will be included by the framework, as far as I understand. Simplified we can say a BOM is specified by modules it uses. In addition you have the option to use variables and providers definitions related to a BOM specification. It is good to know that Modules can have dependencies to other modules, if this is the case the related modules will be included by the framework, as far as I understand. Here is a simplified overview diagram of the dependencies: Here is a simplified activity diagram that shows the activities carried out by the user and the IasCable framework. Realize Software Everywhere with Bill of Material and Modules \u00b6 Simplified we can say, we have two basic roles in that context: Creators (Architect, Developer or Operator) defining Bill of Materials to create Terraform automation for creating specific infrastructure based on reusing existing Terraform modules. Consumers who using the create Terraform automation based on the BOM definition. And we have two major elements to define and create the needed Terraform automation. The BOM configures IasCable to point to right Terraform modules in a module catalog to create the Terraform automation code. IasCable is uses Terraform modules to create a Terraform automation which will be consumed. The following diagram shows some high level dependencies. 2. Pre-requisites for the example \u00b6 Following tools need to be installed on your local computer to follow the step by step instructions. Terraform Git That is the cloud environment we will use. IBM Cloud 3. Step-by-step example setup \u00b6 This is a step by step example setup to create a Virtual Private Cloud with three Subnets on IBM Cloud. 1 x Virtual Private Cloud 3 x Subnets 2 x Access Control Lists 1 x Routing Table 2 x Security Groups Simplified architecture overview Step 1: Install CLI \u00b6 curl -sL https://raw.githubusercontent.com/cloud-native-toolkit/iascable/main/install.sh | sh Step 2: Verify the installation \u00b6 iascable build --help Example output: Configure ( and optionally deploy ) the iteration zero assets Options: --version Show version number [ boolean ] --help Show help [ boolean ] -u, --catalogUrl The url of the module catalog. Can be https:// or file:/ protocol. [ default: \"https://modules.cloudnativetoolkit.dev/index.yaml\" ] -i, --input The path to the bill of materials to use as input [ array ] -r, --reference The reference BOM to use for the build [ array ] -o, --outDir The base directory where the command output will be written [ default: \"./output\" ] --platform Filter for the platform ( kubernetes or ocp4 ) --provider Filter for the provider ( ibm or k8s ) --tileLabel The label for the tile. Required if you want to generate the tile metadata. --name The name used to override the module name in the bill of material. [ array ] --tileDescription The description of the tile. --flattenOutput, --flatten Flatten the generated output into a single directory ( i.e. remove the terraform folder ) . [ boolean ] --debug Flag to turn on more detailed output message [ boolean ] Step 3: Create a Bill of Materials (BOM) file \u00b6 nano firstbom.yaml Copy following content into the new file: apiVersion : cloudnativetoolkit.dev/v1alpha1 kind : BillOfMaterial metadata : name : ibm-vpc spec : modules : - name : ibm-vpc - name : ibm-vpc-subnets Step 4: Execute following command \u00b6 iascable build -i firstbom.yaml Step 5: Verify the created content \u00b6 \u251c\u2500\u2500 firstbom.yaml \u2514\u2500\u2500 output \u251c\u2500\u2500 ibm-vpc \u2502 \u251c\u2500\u2500 apply.sh \u2502 \u251c\u2500\u2500 bom.yaml \u2502 \u251c\u2500\u2500 dependencies.dot \u2502 \u251c\u2500\u2500 destroy.sh \u2502 \u2514\u2500\u2500 terraform \u2502 \u251c\u2500\u2500 docs \u2502 \u2502 \u251c\u2500\u2500 ibm-resource-group.md \u2502 \u2502 \u251c\u2500\u2500 ibm-vpc-subnets.md \u2502 \u2502 \u2514\u2500\u2500 ibm-vpc.md \u2502 \u251c\u2500\u2500 ibm-vpc.auto.tfvars \u2502 \u251c\u2500\u2500 main.tf \u2502 \u251c\u2500\u2500 providers.tf \u2502 \u251c\u2500\u2500 variables.tf \u2502 \u2514\u2500\u2500 version.tf \u2514\u2500\u2500 launch.sh output folder The folder output contains all the content created by the iascable build command output/ibm-vpc folder The folder ibm-vpc is the name we used in our own BOM file. Let us call that folder project folder, which was defined in meta data. metadata : name : ibm-vpc output/ibm-vpc/terraform folder This is table contains the list of files in the terraform folder. Filename Content output/ibm-vpc/terraform/main.tf Here you see a number of modules defined including the defined ibm-vpc and ibm-vpc-subnets from the BOM file. output/ibm-vpc/terraform/providers.tf Simply contains the needed cloud provider information. In that case what we need to specify for IBM Cloud . output/ibm-vpc/terraform/variables.ft Contains the specification for the used variable in the main.tf or other Terraform files. output/ibm-vpc/terraform/version.ft Contains the specification for the used Terraform provider sources and versions. In that case only IBM is listed. output/ibm-vpc/terraform/ibm-vpc.auto.tfvars That file can be used to configure the variable values. (maybe add to .gitignore) During the execution of terraform plan and terraform apply you will be ask for input, if you didn't specify that values. The output/launch.sh file That script downloads and starts a container on your local machine. The objective to ensure that the right environment is used for applying the Terraform configuration. It attaches the local path to the container as a volume. Note: Need to ensure you have a container engine on your machine. Best Docker! Because by default is uses Docker. Attach doesn't work for podman on macOS. The output/ibm-vpc/apply.sh file That file converts an existing variable.yaml file or variable in the BOM file to a variables.tf and then executes terraform init and terraform apply . The output/ibm-vpc/destroy.sh file That file simply executes the terraform init and terraform destroy -auto-approve commands. The output/ibm-vpc/dependencies.dot file That file contains the dependencies which can be visualized for example with Graphviz Online . Example: The output/bom.yaml file That file was created by our own BOM file . That file now contains all needed variables. These variables are also reflected in the output/ibm-vpc/terraform/variables.ft file. Here is the content of the newly created bom.yaml file. apiVersion : cloudnativetoolkit.dev/v1alpha1 kind : BillOfMaterial metadata : name : ibm-vpc spec : modules : - name : ibm-vpc alias : ibm-vpc version : v1.16.0 - name : ibm-vpc-subnets alias : ibm-vpc-subnets version : v1.13.2 - name : ibm-resource-group alias : resource_group version : v3.2.16 variables : - name : region type : string description : The IBM Cloud region where the cluster will be/has been installed. - name : ibmcloud_api_key type : string - name : ibm-vpc-subnets__count type : number description : The number of subnets that should be provisioned defaultValue : 3 - name : resource_group_name type : string description : The name of the resource group Step 6: Execute the terraform init command \u00b6 Navigate to the output/ibm-vpc/terraform folder and execute the terraform init command. cd output/ibm-vpc/terraform terraform init Step 7: Execute the terraform plan command \u00b6 Execute the terraform plan command. terraform plan Here you can see the interaction: var.ibmcloud_api_key the value of ibmcloud_api_key Enter a value: Note: You can create an IBM Cloud API Key with following command: ibmcloud iam api-key-create iascable-example . Step 8: Execute the terraform apply \u00b6 Execute the terraform apply command. terraform apply -auto-approve Input of your variables: var.ibmcloud_api_key the value of ibmcloud_api_key Enter a value: xxx var.region The IBM Cloud region where the cluster will be/has been installed. Enter a value: us-south var.resource_group_name The name of the resource group Enter a value: default Output overview of the resources which will be created: Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create < = read ( data resources ) Terraform will perform the following actions: # module.ibm-vpc.data.ibm_is_security_group.base will be read during apply # (config refers to values not yet known) < = data \"ibm_is_security_group\" \"base\" { + crn = ( known after apply ) + id = ( known after apply ) + name = \"default-vpc-base\" + resource_controller_url = ( known after apply ) + resource_crn = ( known after apply ) + resource_group_name = ( known after apply ) + resource_name = ( known after apply ) + rules = ( known after apply ) + tags = ( known after apply ) + vpc = ( known after apply ) } # module.ibm-vpc.data.ibm_is_vpc.vpc will be read during apply # (config refers to values not yet known) < = data \"ibm_is_vpc\" \"vpc\" { + classic_access = ( known after apply ) + crn = ( known after apply ) + cse_source_addresses = ( known after apply ) + default_network_acl = ( known after apply ) + default_network_acl_crn = ( known after apply ) + default_network_acl_name = ( known after apply ) + default_routing_table = ( known after apply ) + default_routing_table_name = ( known after apply ) + default_security_group = ( known after apply ) + default_security_group_crn = ( known after apply ) + default_security_group_name = ( known after apply ) + id = ( known after apply ) + name = \"default-vpc\" + resource_controller_url = ( known after apply ) + resource_crn = ( known after apply ) + resource_group = ( known after apply ) + resource_group_name = ( known after apply ) + resource_name = ( known after apply ) + resource_status = ( known after apply ) + security_group = ( known after apply ) + status = ( known after apply ) + subnets = ( known after apply ) + tags = ( known after apply ) } # module.ibm-vpc.data.ibm_resource_group.resource_group will be read during apply # (config refers to values not yet known) < = data \"ibm_resource_group\" \"resource_group\" { + account_id = ( known after apply ) + created_at = ( known after apply ) + crn = ( known after apply ) + id = ( known after apply ) + is_default = ( known after apply ) + name = \"default\" + payment_methods_url = ( known after apply ) + quota_id = ( known after apply ) + quota_url = ( known after apply ) + resource_linkages = ( known after apply ) + state = ( known after apply ) + teams_url = ( known after apply ) + updated_at = ( known after apply ) } # module.ibm-vpc.ibm_is_network_acl_rule.allow_internal_egress[0] will be created + resource \"ibm_is_network_acl_rule\" \"allow_internal_egress\" { + action = \"allow\" + before = ( known after apply ) + destination = \"10.0.0.0/8\" + direction = \"outbound\" + href = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = \"allow-internal-egress\" + network_acl = ( known after apply ) + protocol = ( known after apply ) + rule_id = ( known after apply ) + source = \"10.0.0.0/8\" } # module.ibm-vpc.ibm_is_network_acl_rule.allow_internal_ingress[0] will be created + resource \"ibm_is_network_acl_rule\" \"allow_internal_ingress\" { + action = \"allow\" + before = ( known after apply ) + destination = \"10.0.0.0/8\" + direction = \"inbound\" + href = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = \"allow-internal-ingress\" + network_acl = ( known after apply ) + protocol = ( known after apply ) + rule_id = ( known after apply ) + source = \"10.0.0.0/8\" } # module.ibm-vpc.ibm_is_network_acl_rule.deny_external_ingress[0] will be created + resource \"ibm_is_network_acl_rule\" \"deny_external_ingress\" { + action = \"deny\" + before = ( known after apply ) + destination = \"0.0.0.0/0\" + direction = \"inbound\" + href = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = \"deny-external-ingress\" + network_acl = ( known after apply ) + protocol = ( known after apply ) + rule_id = ( known after apply ) + source = \"0.0.0.0/0\" } # module.ibm-vpc.ibm_is_network_acl_rule.deny_external_rdp[0] will be created + resource \"ibm_is_network_acl_rule\" \"deny_external_rdp\" { + action = \"deny\" + before = ( known after apply ) + destination = \"0.0.0.0/0\" + direction = \"inbound\" + href = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = \"deny-external-rdp\" + network_acl = ( known after apply ) + protocol = ( known after apply ) + rule_id = ( known after apply ) + source = \"0.0.0.0/0\" + tcp { + port_max = 3389 + port_min = 3389 + source_port_max = 3389 + source_port_min = 3389 } } # module.ibm-vpc.ibm_is_network_acl_rule.deny_external_ssh[0] will be created + resource \"ibm_is_network_acl_rule\" \"deny_external_ssh\" { + action = \"deny\" + before = ( known after apply ) + destination = \"0.0.0.0/0\" + direction = \"inbound\" + href = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = \"deny-external-ssh\" + network_acl = ( known after apply ) + protocol = ( known after apply ) + rule_id = ( known after apply ) + source = \"0.0.0.0/0\" + tcp { + port_max = 22 + port_min = 22 + source_port_max = 22 + source_port_min = 22 } } # module.ibm-vpc.ibm_is_security_group.base[0] will be created + resource \"ibm_is_security_group\" \"base\" { + crn = ( known after apply ) + id = ( known after apply ) + name = \"default-vpc-base\" + resource_controller_url = ( known after apply ) + resource_crn = ( known after apply ) + resource_group = ( known after apply ) + resource_group_name = ( known after apply ) + resource_name = ( known after apply ) + rules = ( known after apply ) + tags = ( known after apply ) + vpc = ( known after apply ) } # module.ibm-vpc.ibm_is_security_group_rule.cse_dns_1[0] will be created + resource \"ibm_is_security_group_rule\" \"cse_dns_1\" { + direction = \"outbound\" + group = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + protocol = ( known after apply ) + related_crn = ( known after apply ) + remote = \"161.26.0.10\" + rule_id = ( known after apply ) + udp { + port_max = 53 + port_min = 53 } } # module.ibm-vpc.ibm_is_security_group_rule.cse_dns_1[1] will be created + resource \"ibm_is_security_group_rule\" \"cse_dns_1\" { + direction = \"outbound\" + group = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + protocol = ( known after apply ) + related_crn = ( known after apply ) + remote = \"161.26.0.10\" + rule_id = ( known after apply ) + udp { + port_max = 53 + port_min = 53 } } # module.ibm-vpc.ibm_is_security_group_rule.cse_dns_2[0] will be created + resource \"ibm_is_security_group_rule\" \"cse_dns_2\" { + direction = \"outbound\" + group = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + protocol = ( known after apply ) + related_crn = ( known after apply ) + remote = \"161.26.0.11\" + rule_id = ( known after apply ) + udp { + port_max = 53 + port_min = 53 } } # module.ibm-vpc.ibm_is_security_group_rule.cse_dns_2[1] will be created + resource \"ibm_is_security_group_rule\" \"cse_dns_2\" { + direction = \"outbound\" + group = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + protocol = ( known after apply ) + related_crn = ( known after apply ) + remote = \"161.26.0.11\" + rule_id = ( known after apply ) + udp { + port_max = 53 + port_min = 53 } } # module.ibm-vpc.ibm_is_security_group_rule.default_inbound_http[0] will be created + resource \"ibm_is_security_group_rule\" \"default_inbound_http\" { + direction = \"inbound\" + group = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + protocol = ( known after apply ) + related_crn = ( known after apply ) + remote = \"0.0.0.0/0\" + rule_id = ( known after apply ) + tcp { + port_max = 80 + port_min = 80 } } # module.ibm-vpc.ibm_is_security_group_rule.default_inbound_ping[0] will be created + resource \"ibm_is_security_group_rule\" \"default_inbound_ping\" { + direction = \"inbound\" + group = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + protocol = ( known after apply ) + related_crn = ( known after apply ) + remote = \"0.0.0.0/0\" + rule_id = ( known after apply ) + icmp { + type = 8 } } # module.ibm-vpc.ibm_is_security_group_rule.private_dns_1[0] will be created + resource \"ibm_is_security_group_rule\" \"private_dns_1\" { + direction = \"outbound\" + group = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + protocol = ( known after apply ) + related_crn = ( known after apply ) + remote = \"161.26.0.7\" + rule_id = ( known after apply ) + udp { + port_max = 53 + port_min = 53 } } # module.ibm-vpc.ibm_is_security_group_rule.private_dns_1[1] will be created + resource \"ibm_is_security_group_rule\" \"private_dns_1\" { + direction = \"outbound\" + group = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + protocol = ( known after apply ) + related_crn = ( known after apply ) + remote = \"161.26.0.7\" + rule_id = ( known after apply ) + udp { + port_max = 53 + port_min = 53 } } # module.ibm-vpc.ibm_is_security_group_rule.private_dns_2[0] will be created + resource \"ibm_is_security_group_rule\" \"private_dns_2\" { + direction = \"outbound\" + group = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + protocol = ( known after apply ) + related_crn = ( known after apply ) + remote = \"161.26.0.8\" + rule_id = ( known after apply ) + udp { + port_max = 53 + port_min = 53 } } # module.ibm-vpc.ibm_is_security_group_rule.private_dns_2[1] will be created + resource \"ibm_is_security_group_rule\" \"private_dns_2\" { + direction = \"outbound\" + group = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + protocol = ( known after apply ) + related_crn = ( known after apply ) + remote = \"161.26.0.8\" + rule_id = ( known after apply ) + udp { + port_max = 53 + port_min = 53 } } # module.ibm-vpc.ibm_is_vpc.vpc[0] will be created + resource \"ibm_is_vpc\" \"vpc\" { + address_prefix_management = \"auto\" + classic_access = false + crn = ( known after apply ) + cse_source_addresses = ( known after apply ) + default_network_acl = ( known after apply ) + default_network_acl_crn = ( known after apply ) + default_network_acl_name = \"default-vpc-default\" + default_routing_table = ( known after apply ) + default_routing_table_name = \"default-vpc-default\" + default_security_group = ( known after apply ) + default_security_group_crn = ( known after apply ) + default_security_group_name = \"default-vpc-default\" + id = ( known after apply ) + name = \"default-vpc\" + resource_controller_url = ( known after apply ) + resource_crn = ( known after apply ) + resource_group = ( known after apply ) + resource_group_name = ( known after apply ) + resource_name = ( known after apply ) + resource_status = ( known after apply ) + security_group = ( known after apply ) + status = ( known after apply ) + subnets = ( known after apply ) + tags = ( known after apply ) } # module.ibm-vpc.ibm_resource_tag.nacl-tag[0] will be created + resource \"ibm_resource_tag\" \"nacl-tag\" { + acccount_id = ( known after apply ) + id = ( known after apply ) + resource_id = ( known after apply ) + tag_type = ( known after apply ) + tags = ( known after apply ) } # module.ibm-vpc.ibm_resource_tag.sg-tag[0] will be created + resource \"ibm_resource_tag\" \"sg-tag\" { + acccount_id = ( known after apply ) + id = ( known after apply ) + resource_id = ( known after apply ) + tag_type = ( known after apply ) + tags = ( known after apply ) } # module.ibm-vpc.null_resource.print_names will be created + resource \"null_resource\" \"print_names\" { + id = ( known after apply ) } # module.ibm-vpc-subnets.data.ibm_is_vpc.vpc will be read during apply # (config refers to values not yet known) < = data \"ibm_is_vpc\" \"vpc\" { + classic_access = ( known after apply ) + crn = ( known after apply ) + cse_source_addresses = ( known after apply ) + default_network_acl = ( known after apply ) + default_network_acl_crn = ( known after apply ) + default_network_acl_name = ( known after apply ) + default_routing_table = ( known after apply ) + default_routing_table_name = ( known after apply ) + default_security_group = ( known after apply ) + default_security_group_crn = ( known after apply ) + default_security_group_name = ( known after apply ) + id = ( known after apply ) + name = \"default-vpc\" + resource_controller_url = ( known after apply ) + resource_crn = ( known after apply ) + resource_group = ( known after apply ) + resource_group_name = ( known after apply ) + resource_name = ( known after apply ) + resource_status = ( known after apply ) + security_group = ( known after apply ) + status = ( known after apply ) + subnets = ( known after apply ) + tags = ( known after apply ) } # module.ibm-vpc-subnets.data.ibm_resource_group.resource_group will be read during apply # (config refers to values not yet known) < = data \"ibm_resource_group\" \"resource_group\" { + account_id = ( known after apply ) + created_at = ( known after apply ) + crn = ( known after apply ) + id = ( known after apply ) + is_default = ( known after apply ) + name = \"default\" + payment_methods_url = ( known after apply ) + quota_id = ( known after apply ) + quota_url = ( known after apply ) + resource_linkages = ( known after apply ) + state = ( known after apply ) + teams_url = ( known after apply ) + updated_at = ( known after apply ) } # module.ibm-vpc-subnets.ibm_is_network_acl.subnet_acl[0] will be created + resource \"ibm_is_network_acl\" \"subnet_acl\" { + crn = ( known after apply ) + id = ( known after apply ) + name = \"default-vpc-subnet-default\" + resource_controller_url = ( known after apply ) + resource_crn = ( known after apply ) + resource_group = ( known after apply ) + resource_group_name = ( known after apply ) + resource_name = ( known after apply ) + tags = ( known after apply ) + vpc = ( known after apply ) + rules { + action = ( known after apply ) + destination = ( known after apply ) + direction = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = ( known after apply ) + source = ( known after apply ) + subnets = ( known after apply ) + icmp { + code = ( known after apply ) + type = ( known after apply ) } + tcp { + port_max = ( known after apply ) + port_min = ( known after apply ) + source_port_max = ( known after apply ) + source_port_min = ( known after apply ) } + udp { + port_max = ( known after apply ) + port_min = ( known after apply ) + source_port_max = ( known after apply ) + source_port_min = ( known after apply ) } } } # module.ibm-vpc-subnets.ibm_is_network_acl_rule.acl_rule[0] will be created + resource \"ibm_is_network_acl_rule\" \"acl_rule\" { + action = \"allow\" + before = ( known after apply ) + destination = \"10.0.0.0/8\" + direction = \"inbound\" + href = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = \"default-vpc-subnet-default-allow-ingress-internal\" + network_acl = ( known after apply ) + protocol = ( known after apply ) + rule_id = ( known after apply ) + source = \"10.0.0.0/8\" } # module.ibm-vpc-subnets.ibm_is_network_acl_rule.acl_rule[1] will be created + resource \"ibm_is_network_acl_rule\" \"acl_rule\" { + action = \"allow\" + before = ( known after apply ) + destination = \"10.0.0.0/8\" + direction = \"inbound\" + href = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = \"default-vpc-subnet-default-allow-roks-ingress\" + network_acl = ( known after apply ) + protocol = ( known after apply ) + rule_id = ( known after apply ) + source = \"166.8.0.0/14\" } # module.ibm-vpc-subnets.ibm_is_network_acl_rule.acl_rule[2] will be created + resource \"ibm_is_network_acl_rule\" \"acl_rule\" { + action = \"allow\" + before = ( known after apply ) + destination = \"10.0.0.0/8\" + direction = \"inbound\" + href = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = \"default-vpc-subnet-default-allow-vse-ingress\" + network_acl = ( known after apply ) + protocol = ( known after apply ) + rule_id = ( known after apply ) + source = \"161.26.0.0/16\" } # module.ibm-vpc-subnets.ibm_is_network_acl_rule.acl_rule[3] will be created + resource \"ibm_is_network_acl_rule\" \"acl_rule\" { + action = \"allow\" + before = ( known after apply ) + destination = \"10.0.0.0/8\" + direction = \"outbound\" + href = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = \"default-vpc-subnet-default-allow-egress-internal\" + network_acl = ( known after apply ) + protocol = ( known after apply ) + rule_id = ( known after apply ) + source = \"10.0.0.0/8\" } # module.ibm-vpc-subnets.ibm_is_network_acl_rule.acl_rule[4] will be created + resource \"ibm_is_network_acl_rule\" \"acl_rule\" { + action = \"allow\" + before = ( known after apply ) + destination = \"166.8.0.0/14\" + direction = \"outbound\" + href = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = \"default-vpc-subnet-default-allow-roks-egress\" + network_acl = ( known after apply ) + protocol = ( known after apply ) + rule_id = ( known after apply ) + source = \"10.0.0.0/8\" } # module.ibm-vpc-subnets.ibm_is_network_acl_rule.acl_rule[5] will be created + resource \"ibm_is_network_acl_rule\" \"acl_rule\" { + action = \"allow\" + before = ( known after apply ) + destination = \"161.26.0.0/16\" + direction = \"outbound\" + href = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = \"default-vpc-subnet-default-allow-vse-egress\" + network_acl = ( known after apply ) + protocol = ( known after apply ) + rule_id = ( known after apply ) + source = \"10.0.0.0/8\" } # module.ibm-vpc-subnets.ibm_is_subnet.vpc_subnets[0] will be created + resource \"ibm_is_subnet\" \"vpc_subnets\" { + access_tags = ( known after apply ) + available_ipv4_address_count = ( known after apply ) + crn = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + ipv4_cidr_block = ( known after apply ) + name = \"default-vpc-subnet-default01\" + network_acl = ( known after apply ) + public_gateway = ( known after apply ) + resource_controller_url = ( known after apply ) + resource_crn = ( known after apply ) + resource_group = ( known after apply ) + resource_group_name = ( known after apply ) + resource_name = ( known after apply ) + resource_status = ( known after apply ) + routing_table = ( known after apply ) + status = ( known after apply ) + tags = ( known after apply ) + total_ipv4_address_count = 256 + vpc = ( known after apply ) + zone = \"us-south-1\" } # module.ibm-vpc-subnets.ibm_is_subnet.vpc_subnets[1] will be created + resource \"ibm_is_subnet\" \"vpc_subnets\" { + access_tags = ( known after apply ) + available_ipv4_address_count = ( known after apply ) + crn = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + ipv4_cidr_block = ( known after apply ) + name = \"default-vpc-subnet-default02\" + network_acl = ( known after apply ) + public_gateway = ( known after apply ) + resource_controller_url = ( known after apply ) + resource_crn = ( known after apply ) + resource_group = ( known after apply ) + resource_group_name = ( known after apply ) + resource_name = ( known after apply ) + resource_status = ( known after apply ) + routing_table = ( known after apply ) + status = ( known after apply ) + tags = ( known after apply ) + total_ipv4_address_count = 256 + vpc = ( known after apply ) + zone = \"us-south-2\" } # module.ibm-vpc-subnets.ibm_is_subnet.vpc_subnets[2] will be created + resource \"ibm_is_subnet\" \"vpc_subnets\" { + access_tags = ( known after apply ) + available_ipv4_address_count = ( known after apply ) + crn = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + ipv4_cidr_block = ( known after apply ) + name = \"default-vpc-subnet-default03\" + network_acl = ( known after apply ) + public_gateway = ( known after apply ) + resource_controller_url = ( known after apply ) + resource_crn = ( known after apply ) + resource_group = ( known after apply ) + resource_group_name = ( known after apply ) + resource_name = ( known after apply ) + resource_status = ( known after apply ) + routing_table = ( known after apply ) + status = ( known after apply ) + tags = ( known after apply ) + total_ipv4_address_count = 256 + vpc = ( known after apply ) + zone = \"us-south-3\" } # module.ibm-vpc-subnets.null_resource.print_names will be created + resource \"null_resource\" \"print_names\" { + id = ( known after apply ) } # module.resource_group.data.ibm_resource_group.resource_group will be read during apply # (config refers to values not yet known) < = data \"ibm_resource_group\" \"resource_group\" { + account_id = ( known after apply ) + created_at = ( known after apply ) + crn = ( known after apply ) + id = ( known after apply ) + is_default = ( known after apply ) + name = \"default\" + payment_methods_url = ( known after apply ) + quota_id = ( known after apply ) + quota_url = ( known after apply ) + resource_linkages = ( known after apply ) + state = ( known after apply ) + teams_url = ( known after apply ) + updated_at = ( known after apply ) } # module.resource_group.data.ibm_resource_tag.resource_group_tags will be read during apply # (config refers to values not yet known) < = data \"ibm_resource_tag\" \"resource_group_tags\" { + id = ( known after apply ) + resource_id = ( known after apply ) + tags = ( known after apply ) } # module.resource_group.null_resource.resource_group will be created + resource \"null_resource\" \"resource_group\" { + id = ( known after apply ) + triggers = ( known after apply ) } # module.resource_group.null_resource.wait_for_sync will be created + resource \"null_resource\" \"wait_for_sync\" { + id = ( known after apply ) } # module.resource_group.random_uuid.tag will be created + resource \"random_uuid\" \"tag\" { + id = ( known after apply ) + result = ( known after apply ) } # module.resource_group.module.clis.data.external.setup-binaries will be read during apply # (config refers to values not yet known) < = data \"external\" \"setup-binaries\" { + id = ( known after apply ) + program = [ + \"bash\" , + \".terraform/modules/resource_group.clis/scripts/setup-binaries.sh\" , ] + query = { + \"bin_dir\" = \"/Users/thomassuedbroecker/Downloads/dev/iascable-starting-point/examples/output/ibm-vpc/terraform/bin2\" + \"clis\" = \"yq,jq,igc\" + \"uuid\" = ( known after apply ) } + result = ( known after apply ) } # module.resource_group.module.clis.null_resource.print will be created + resource \"null_resource\" \"print\" { + id = ( known after apply ) } # module.resource_group.module.clis.random_string.uuid will be created + resource \"random_string\" \"uuid\" { + id = ( known after apply ) + length = 16 + lower = true + min_lower = 0 + min_numeric = 0 + min_special = 0 + min_upper = 0 + number = false + numeric = ( known after apply ) + result = ( known after apply ) + special = false + upper = false } Plan: 36 to add, 0 to change, 0 to destroy. \u2577 \u2502 Warning: Experimental feature \"module_variable_optional_attrs\" is active \u2502 \u2502 on .terraform/modules/ibm-vpc-subnets/version.tf line 10 , in terraform: \u2502 10 : experiments = [ module_variable_optional_attrs ] \u2502 \u2502 Experimental features are subject to breaking changes in \u2502 future minor or patch releases, based on feedback. \u2502 \u2502 If you have feedback on the design of this feature, please \u2502 open a GitHub issue to discuss it. \u2575 \u2577 \u2502 Warning: Argument is deprecated \u2502 \u2502 with module.resource_group.module.clis.random_string.uuid, \u2502 on .terraform/modules/resource_group.clis/main.tf line 15 , in resource \"random_string\" \"uuid\" : \u2502 15 : number = false \u2502 \u2502 Use numeric instead. \u2502 \u2502 ( and one more similar warning elsewhere ) Do you want to perform these actions? Terraform will perform the actions described above. Only 'yes' will be accepted to approve. Enter a value: yes Final result of the creation Apply complete! Resources: 36 added, 0 changed, 0 destroyed. Step 9: Execute the terraform destroy command \u00b6 Note: Ensure you didn't delete the terraform.tfstate and the .terraform.lock.hcl files before. To destroy the provisioned resources, run the following: terraform destroy -auto-approve You need to ensure to provide the IBM Cloud API Key, the region and the resource group name. var.ibmcloud_api_key the value of ibmcloud_api_key Enter a value: xxxx var.region The IBM Cloud region where the cluster will be/has been installed. Enter a value: us-south var.resource_group_name The name of the resource group Enter a value: default Output overview: \u2577 \u2502 Warning: Experimental feature \"module_variable_optional_attrs\" is active \u2502 \u2502 on .terraform/modules/ibm-vpc-subnets/version.tf line 10 , in terraform: \u2502 10 : experiments = [ module_variable_optional_attrs ] \u2502 \u2502 Experimental features are subject to breaking changes in \u2502 future minor or patch releases, based on feedback. \u2502 \u2502 If you have feedback on the design of this feature, please \u2502 open a GitHub issue to discuss it. \u2575 \u2577 \u2502 Warning: Argument is deprecated \u2502 \u2502 with module.resource_group.module.clis.random_string.uuid, \u2502 on .terraform/modules/resource_group.clis/main.tf line 15 , in resource \"random_string\" \"uuid\" : \u2502 15 : number = false \u2502 \u2502 Use numeric instead. \u2502 \u2502 ( and one more similar warning elsewhere ) \u2575 Destroy complete! Resources: 36 destroyed. 4. Summary \u00b6 The IasCable and the Modules (from the Software Everywhere project) which are all from the Cloud Native Toolkit providing a good concept for a framework to provide reusable components to install and configure cloud infrastructure. This was just a getting started. There is more to learn.","title":"Lab 1 - Getting started with the basics"},{"location":"getting-started/lab1/#lab-1-getting-start-with-the-basics","text":"That lab project uses following resource as input: From IasCable and that blog post . This is just a starting point to use IasCable .","title":"Lab 1: Getting start with the basics"},{"location":"getting-started/lab1/#1-the-iascable-framework","text":"Let us have first a look the basic components of the IasCable framework.","title":"1. The IasCable framework"},{"location":"getting-started/lab1/#bill-of-material-and-modules","text":"The IasCable uses a Bill of Material and Modules (from the Software Everywhere project ), which you need to understand. These two parts are the heart of the framework we could say to realize the objective to build an installable component infrastructure based on components from a catalog of available modules. Please visit the linked resources for more details. Simplified we can say a by modules it uses, variables you can define and providers you can define. It is good to know that modules can depend on other modules, if this is the case the related modules will be included by the framework, as far as I understand. Simplified we can say a BOM is specified by modules it uses. In addition you have the option to use variables and providers definitions related to a BOM specification. It is good to know that Modules can have dependencies to other modules, if this is the case the related modules will be included by the framework, as far as I understand. Here is a simplified overview diagram of the dependencies: Here is a simplified activity diagram that shows the activities carried out by the user and the IasCable framework.","title":"Bill of Material and Modules"},{"location":"getting-started/lab1/#realize-software-everywhere-with-bill-of-material-and-modules","text":"Simplified we can say, we have two basic roles in that context: Creators (Architect, Developer or Operator) defining Bill of Materials to create Terraform automation for creating specific infrastructure based on reusing existing Terraform modules. Consumers who using the create Terraform automation based on the BOM definition. And we have two major elements to define and create the needed Terraform automation. The BOM configures IasCable to point to right Terraform modules in a module catalog to create the Terraform automation code. IasCable is uses Terraform modules to create a Terraform automation which will be consumed. The following diagram shows some high level dependencies.","title":"Realize Software Everywhere with Bill of Material and Modules"},{"location":"getting-started/lab1/#2-pre-requisites-for-the-example","text":"Following tools need to be installed on your local computer to follow the step by step instructions. Terraform Git That is the cloud environment we will use. IBM Cloud","title":"2. Pre-requisites for the example"},{"location":"getting-started/lab1/#3-step-by-step-example-setup","text":"This is a step by step example setup to create a Virtual Private Cloud with three Subnets on IBM Cloud. 1 x Virtual Private Cloud 3 x Subnets 2 x Access Control Lists 1 x Routing Table 2 x Security Groups Simplified architecture overview","title":"3. Step-by-step example setup"},{"location":"getting-started/lab1/#step-1-install-cli","text":"curl -sL https://raw.githubusercontent.com/cloud-native-toolkit/iascable/main/install.sh | sh","title":"Step 1: Install CLI"},{"location":"getting-started/lab1/#step-2-verify-the-installation","text":"iascable build --help Example output: Configure ( and optionally deploy ) the iteration zero assets Options: --version Show version number [ boolean ] --help Show help [ boolean ] -u, --catalogUrl The url of the module catalog. Can be https:// or file:/ protocol. [ default: \"https://modules.cloudnativetoolkit.dev/index.yaml\" ] -i, --input The path to the bill of materials to use as input [ array ] -r, --reference The reference BOM to use for the build [ array ] -o, --outDir The base directory where the command output will be written [ default: \"./output\" ] --platform Filter for the platform ( kubernetes or ocp4 ) --provider Filter for the provider ( ibm or k8s ) --tileLabel The label for the tile. Required if you want to generate the tile metadata. --name The name used to override the module name in the bill of material. [ array ] --tileDescription The description of the tile. --flattenOutput, --flatten Flatten the generated output into a single directory ( i.e. remove the terraform folder ) . [ boolean ] --debug Flag to turn on more detailed output message [ boolean ]","title":"Step 2: Verify the installation"},{"location":"getting-started/lab1/#step-3-create-a-bill-of-materialsbom-file","text":"nano firstbom.yaml Copy following content into the new file: apiVersion : cloudnativetoolkit.dev/v1alpha1 kind : BillOfMaterial metadata : name : ibm-vpc spec : modules : - name : ibm-vpc - name : ibm-vpc-subnets","title":"Step 3: Create a Bill of Materials(BOM) file"},{"location":"getting-started/lab1/#step-4-execute-following-command","text":"iascable build -i firstbom.yaml","title":"Step 4: Execute following command"},{"location":"getting-started/lab1/#step-5-verify-the-created-content","text":"\u251c\u2500\u2500 firstbom.yaml \u2514\u2500\u2500 output \u251c\u2500\u2500 ibm-vpc \u2502 \u251c\u2500\u2500 apply.sh \u2502 \u251c\u2500\u2500 bom.yaml \u2502 \u251c\u2500\u2500 dependencies.dot \u2502 \u251c\u2500\u2500 destroy.sh \u2502 \u2514\u2500\u2500 terraform \u2502 \u251c\u2500\u2500 docs \u2502 \u2502 \u251c\u2500\u2500 ibm-resource-group.md \u2502 \u2502 \u251c\u2500\u2500 ibm-vpc-subnets.md \u2502 \u2502 \u2514\u2500\u2500 ibm-vpc.md \u2502 \u251c\u2500\u2500 ibm-vpc.auto.tfvars \u2502 \u251c\u2500\u2500 main.tf \u2502 \u251c\u2500\u2500 providers.tf \u2502 \u251c\u2500\u2500 variables.tf \u2502 \u2514\u2500\u2500 version.tf \u2514\u2500\u2500 launch.sh output folder The folder output contains all the content created by the iascable build command output/ibm-vpc folder The folder ibm-vpc is the name we used in our own BOM file. Let us call that folder project folder, which was defined in meta data. metadata : name : ibm-vpc output/ibm-vpc/terraform folder This is table contains the list of files in the terraform folder. Filename Content output/ibm-vpc/terraform/main.tf Here you see a number of modules defined including the defined ibm-vpc and ibm-vpc-subnets from the BOM file. output/ibm-vpc/terraform/providers.tf Simply contains the needed cloud provider information. In that case what we need to specify for IBM Cloud . output/ibm-vpc/terraform/variables.ft Contains the specification for the used variable in the main.tf or other Terraform files. output/ibm-vpc/terraform/version.ft Contains the specification for the used Terraform provider sources and versions. In that case only IBM is listed. output/ibm-vpc/terraform/ibm-vpc.auto.tfvars That file can be used to configure the variable values. (maybe add to .gitignore) During the execution of terraform plan and terraform apply you will be ask for input, if you didn't specify that values. The output/launch.sh file That script downloads and starts a container on your local machine. The objective to ensure that the right environment is used for applying the Terraform configuration. It attaches the local path to the container as a volume. Note: Need to ensure you have a container engine on your machine. Best Docker! Because by default is uses Docker. Attach doesn't work for podman on macOS. The output/ibm-vpc/apply.sh file That file converts an existing variable.yaml file or variable in the BOM file to a variables.tf and then executes terraform init and terraform apply . The output/ibm-vpc/destroy.sh file That file simply executes the terraform init and terraform destroy -auto-approve commands. The output/ibm-vpc/dependencies.dot file That file contains the dependencies which can be visualized for example with Graphviz Online . Example: The output/bom.yaml file That file was created by our own BOM file . That file now contains all needed variables. These variables are also reflected in the output/ibm-vpc/terraform/variables.ft file. Here is the content of the newly created bom.yaml file. apiVersion : cloudnativetoolkit.dev/v1alpha1 kind : BillOfMaterial metadata : name : ibm-vpc spec : modules : - name : ibm-vpc alias : ibm-vpc version : v1.16.0 - name : ibm-vpc-subnets alias : ibm-vpc-subnets version : v1.13.2 - name : ibm-resource-group alias : resource_group version : v3.2.16 variables : - name : region type : string description : The IBM Cloud region where the cluster will be/has been installed. - name : ibmcloud_api_key type : string - name : ibm-vpc-subnets__count type : number description : The number of subnets that should be provisioned defaultValue : 3 - name : resource_group_name type : string description : The name of the resource group","title":"Step 5: Verify the created content"},{"location":"getting-started/lab1/#step-6-execute-the-terraform-init-command","text":"Navigate to the output/ibm-vpc/terraform folder and execute the terraform init command. cd output/ibm-vpc/terraform terraform init","title":"Step 6: Execute the terraform init command"},{"location":"getting-started/lab1/#step-7-execute-the-terraform-plan-command","text":"Execute the terraform plan command. terraform plan Here you can see the interaction: var.ibmcloud_api_key the value of ibmcloud_api_key Enter a value: Note: You can create an IBM Cloud API Key with following command: ibmcloud iam api-key-create iascable-example .","title":"Step 7: Execute the terraform plan  command"},{"location":"getting-started/lab1/#step-8-execute-the-terraform-apply","text":"Execute the terraform apply command. terraform apply -auto-approve Input of your variables: var.ibmcloud_api_key the value of ibmcloud_api_key Enter a value: xxx var.region The IBM Cloud region where the cluster will be/has been installed. Enter a value: us-south var.resource_group_name The name of the resource group Enter a value: default Output overview of the resources which will be created: Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create < = read ( data resources ) Terraform will perform the following actions: # module.ibm-vpc.data.ibm_is_security_group.base will be read during apply # (config refers to values not yet known) < = data \"ibm_is_security_group\" \"base\" { + crn = ( known after apply ) + id = ( known after apply ) + name = \"default-vpc-base\" + resource_controller_url = ( known after apply ) + resource_crn = ( known after apply ) + resource_group_name = ( known after apply ) + resource_name = ( known after apply ) + rules = ( known after apply ) + tags = ( known after apply ) + vpc = ( known after apply ) } # module.ibm-vpc.data.ibm_is_vpc.vpc will be read during apply # (config refers to values not yet known) < = data \"ibm_is_vpc\" \"vpc\" { + classic_access = ( known after apply ) + crn = ( known after apply ) + cse_source_addresses = ( known after apply ) + default_network_acl = ( known after apply ) + default_network_acl_crn = ( known after apply ) + default_network_acl_name = ( known after apply ) + default_routing_table = ( known after apply ) + default_routing_table_name = ( known after apply ) + default_security_group = ( known after apply ) + default_security_group_crn = ( known after apply ) + default_security_group_name = ( known after apply ) + id = ( known after apply ) + name = \"default-vpc\" + resource_controller_url = ( known after apply ) + resource_crn = ( known after apply ) + resource_group = ( known after apply ) + resource_group_name = ( known after apply ) + resource_name = ( known after apply ) + resource_status = ( known after apply ) + security_group = ( known after apply ) + status = ( known after apply ) + subnets = ( known after apply ) + tags = ( known after apply ) } # module.ibm-vpc.data.ibm_resource_group.resource_group will be read during apply # (config refers to values not yet known) < = data \"ibm_resource_group\" \"resource_group\" { + account_id = ( known after apply ) + created_at = ( known after apply ) + crn = ( known after apply ) + id = ( known after apply ) + is_default = ( known after apply ) + name = \"default\" + payment_methods_url = ( known after apply ) + quota_id = ( known after apply ) + quota_url = ( known after apply ) + resource_linkages = ( known after apply ) + state = ( known after apply ) + teams_url = ( known after apply ) + updated_at = ( known after apply ) } # module.ibm-vpc.ibm_is_network_acl_rule.allow_internal_egress[0] will be created + resource \"ibm_is_network_acl_rule\" \"allow_internal_egress\" { + action = \"allow\" + before = ( known after apply ) + destination = \"10.0.0.0/8\" + direction = \"outbound\" + href = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = \"allow-internal-egress\" + network_acl = ( known after apply ) + protocol = ( known after apply ) + rule_id = ( known after apply ) + source = \"10.0.0.0/8\" } # module.ibm-vpc.ibm_is_network_acl_rule.allow_internal_ingress[0] will be created + resource \"ibm_is_network_acl_rule\" \"allow_internal_ingress\" { + action = \"allow\" + before = ( known after apply ) + destination = \"10.0.0.0/8\" + direction = \"inbound\" + href = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = \"allow-internal-ingress\" + network_acl = ( known after apply ) + protocol = ( known after apply ) + rule_id = ( known after apply ) + source = \"10.0.0.0/8\" } # module.ibm-vpc.ibm_is_network_acl_rule.deny_external_ingress[0] will be created + resource \"ibm_is_network_acl_rule\" \"deny_external_ingress\" { + action = \"deny\" + before = ( known after apply ) + destination = \"0.0.0.0/0\" + direction = \"inbound\" + href = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = \"deny-external-ingress\" + network_acl = ( known after apply ) + protocol = ( known after apply ) + rule_id = ( known after apply ) + source = \"0.0.0.0/0\" } # module.ibm-vpc.ibm_is_network_acl_rule.deny_external_rdp[0] will be created + resource \"ibm_is_network_acl_rule\" \"deny_external_rdp\" { + action = \"deny\" + before = ( known after apply ) + destination = \"0.0.0.0/0\" + direction = \"inbound\" + href = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = \"deny-external-rdp\" + network_acl = ( known after apply ) + protocol = ( known after apply ) + rule_id = ( known after apply ) + source = \"0.0.0.0/0\" + tcp { + port_max = 3389 + port_min = 3389 + source_port_max = 3389 + source_port_min = 3389 } } # module.ibm-vpc.ibm_is_network_acl_rule.deny_external_ssh[0] will be created + resource \"ibm_is_network_acl_rule\" \"deny_external_ssh\" { + action = \"deny\" + before = ( known after apply ) + destination = \"0.0.0.0/0\" + direction = \"inbound\" + href = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = \"deny-external-ssh\" + network_acl = ( known after apply ) + protocol = ( known after apply ) + rule_id = ( known after apply ) + source = \"0.0.0.0/0\" + tcp { + port_max = 22 + port_min = 22 + source_port_max = 22 + source_port_min = 22 } } # module.ibm-vpc.ibm_is_security_group.base[0] will be created + resource \"ibm_is_security_group\" \"base\" { + crn = ( known after apply ) + id = ( known after apply ) + name = \"default-vpc-base\" + resource_controller_url = ( known after apply ) + resource_crn = ( known after apply ) + resource_group = ( known after apply ) + resource_group_name = ( known after apply ) + resource_name = ( known after apply ) + rules = ( known after apply ) + tags = ( known after apply ) + vpc = ( known after apply ) } # module.ibm-vpc.ibm_is_security_group_rule.cse_dns_1[0] will be created + resource \"ibm_is_security_group_rule\" \"cse_dns_1\" { + direction = \"outbound\" + group = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + protocol = ( known after apply ) + related_crn = ( known after apply ) + remote = \"161.26.0.10\" + rule_id = ( known after apply ) + udp { + port_max = 53 + port_min = 53 } } # module.ibm-vpc.ibm_is_security_group_rule.cse_dns_1[1] will be created + resource \"ibm_is_security_group_rule\" \"cse_dns_1\" { + direction = \"outbound\" + group = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + protocol = ( known after apply ) + related_crn = ( known after apply ) + remote = \"161.26.0.10\" + rule_id = ( known after apply ) + udp { + port_max = 53 + port_min = 53 } } # module.ibm-vpc.ibm_is_security_group_rule.cse_dns_2[0] will be created + resource \"ibm_is_security_group_rule\" \"cse_dns_2\" { + direction = \"outbound\" + group = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + protocol = ( known after apply ) + related_crn = ( known after apply ) + remote = \"161.26.0.11\" + rule_id = ( known after apply ) + udp { + port_max = 53 + port_min = 53 } } # module.ibm-vpc.ibm_is_security_group_rule.cse_dns_2[1] will be created + resource \"ibm_is_security_group_rule\" \"cse_dns_2\" { + direction = \"outbound\" + group = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + protocol = ( known after apply ) + related_crn = ( known after apply ) + remote = \"161.26.0.11\" + rule_id = ( known after apply ) + udp { + port_max = 53 + port_min = 53 } } # module.ibm-vpc.ibm_is_security_group_rule.default_inbound_http[0] will be created + resource \"ibm_is_security_group_rule\" \"default_inbound_http\" { + direction = \"inbound\" + group = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + protocol = ( known after apply ) + related_crn = ( known after apply ) + remote = \"0.0.0.0/0\" + rule_id = ( known after apply ) + tcp { + port_max = 80 + port_min = 80 } } # module.ibm-vpc.ibm_is_security_group_rule.default_inbound_ping[0] will be created + resource \"ibm_is_security_group_rule\" \"default_inbound_ping\" { + direction = \"inbound\" + group = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + protocol = ( known after apply ) + related_crn = ( known after apply ) + remote = \"0.0.0.0/0\" + rule_id = ( known after apply ) + icmp { + type = 8 } } # module.ibm-vpc.ibm_is_security_group_rule.private_dns_1[0] will be created + resource \"ibm_is_security_group_rule\" \"private_dns_1\" { + direction = \"outbound\" + group = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + protocol = ( known after apply ) + related_crn = ( known after apply ) + remote = \"161.26.0.7\" + rule_id = ( known after apply ) + udp { + port_max = 53 + port_min = 53 } } # module.ibm-vpc.ibm_is_security_group_rule.private_dns_1[1] will be created + resource \"ibm_is_security_group_rule\" \"private_dns_1\" { + direction = \"outbound\" + group = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + protocol = ( known after apply ) + related_crn = ( known after apply ) + remote = \"161.26.0.7\" + rule_id = ( known after apply ) + udp { + port_max = 53 + port_min = 53 } } # module.ibm-vpc.ibm_is_security_group_rule.private_dns_2[0] will be created + resource \"ibm_is_security_group_rule\" \"private_dns_2\" { + direction = \"outbound\" + group = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + protocol = ( known after apply ) + related_crn = ( known after apply ) + remote = \"161.26.0.8\" + rule_id = ( known after apply ) + udp { + port_max = 53 + port_min = 53 } } # module.ibm-vpc.ibm_is_security_group_rule.private_dns_2[1] will be created + resource \"ibm_is_security_group_rule\" \"private_dns_2\" { + direction = \"outbound\" + group = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + protocol = ( known after apply ) + related_crn = ( known after apply ) + remote = \"161.26.0.8\" + rule_id = ( known after apply ) + udp { + port_max = 53 + port_min = 53 } } # module.ibm-vpc.ibm_is_vpc.vpc[0] will be created + resource \"ibm_is_vpc\" \"vpc\" { + address_prefix_management = \"auto\" + classic_access = false + crn = ( known after apply ) + cse_source_addresses = ( known after apply ) + default_network_acl = ( known after apply ) + default_network_acl_crn = ( known after apply ) + default_network_acl_name = \"default-vpc-default\" + default_routing_table = ( known after apply ) + default_routing_table_name = \"default-vpc-default\" + default_security_group = ( known after apply ) + default_security_group_crn = ( known after apply ) + default_security_group_name = \"default-vpc-default\" + id = ( known after apply ) + name = \"default-vpc\" + resource_controller_url = ( known after apply ) + resource_crn = ( known after apply ) + resource_group = ( known after apply ) + resource_group_name = ( known after apply ) + resource_name = ( known after apply ) + resource_status = ( known after apply ) + security_group = ( known after apply ) + status = ( known after apply ) + subnets = ( known after apply ) + tags = ( known after apply ) } # module.ibm-vpc.ibm_resource_tag.nacl-tag[0] will be created + resource \"ibm_resource_tag\" \"nacl-tag\" { + acccount_id = ( known after apply ) + id = ( known after apply ) + resource_id = ( known after apply ) + tag_type = ( known after apply ) + tags = ( known after apply ) } # module.ibm-vpc.ibm_resource_tag.sg-tag[0] will be created + resource \"ibm_resource_tag\" \"sg-tag\" { + acccount_id = ( known after apply ) + id = ( known after apply ) + resource_id = ( known after apply ) + tag_type = ( known after apply ) + tags = ( known after apply ) } # module.ibm-vpc.null_resource.print_names will be created + resource \"null_resource\" \"print_names\" { + id = ( known after apply ) } # module.ibm-vpc-subnets.data.ibm_is_vpc.vpc will be read during apply # (config refers to values not yet known) < = data \"ibm_is_vpc\" \"vpc\" { + classic_access = ( known after apply ) + crn = ( known after apply ) + cse_source_addresses = ( known after apply ) + default_network_acl = ( known after apply ) + default_network_acl_crn = ( known after apply ) + default_network_acl_name = ( known after apply ) + default_routing_table = ( known after apply ) + default_routing_table_name = ( known after apply ) + default_security_group = ( known after apply ) + default_security_group_crn = ( known after apply ) + default_security_group_name = ( known after apply ) + id = ( known after apply ) + name = \"default-vpc\" + resource_controller_url = ( known after apply ) + resource_crn = ( known after apply ) + resource_group = ( known after apply ) + resource_group_name = ( known after apply ) + resource_name = ( known after apply ) + resource_status = ( known after apply ) + security_group = ( known after apply ) + status = ( known after apply ) + subnets = ( known after apply ) + tags = ( known after apply ) } # module.ibm-vpc-subnets.data.ibm_resource_group.resource_group will be read during apply # (config refers to values not yet known) < = data \"ibm_resource_group\" \"resource_group\" { + account_id = ( known after apply ) + created_at = ( known after apply ) + crn = ( known after apply ) + id = ( known after apply ) + is_default = ( known after apply ) + name = \"default\" + payment_methods_url = ( known after apply ) + quota_id = ( known after apply ) + quota_url = ( known after apply ) + resource_linkages = ( known after apply ) + state = ( known after apply ) + teams_url = ( known after apply ) + updated_at = ( known after apply ) } # module.ibm-vpc-subnets.ibm_is_network_acl.subnet_acl[0] will be created + resource \"ibm_is_network_acl\" \"subnet_acl\" { + crn = ( known after apply ) + id = ( known after apply ) + name = \"default-vpc-subnet-default\" + resource_controller_url = ( known after apply ) + resource_crn = ( known after apply ) + resource_group = ( known after apply ) + resource_group_name = ( known after apply ) + resource_name = ( known after apply ) + tags = ( known after apply ) + vpc = ( known after apply ) + rules { + action = ( known after apply ) + destination = ( known after apply ) + direction = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = ( known after apply ) + source = ( known after apply ) + subnets = ( known after apply ) + icmp { + code = ( known after apply ) + type = ( known after apply ) } + tcp { + port_max = ( known after apply ) + port_min = ( known after apply ) + source_port_max = ( known after apply ) + source_port_min = ( known after apply ) } + udp { + port_max = ( known after apply ) + port_min = ( known after apply ) + source_port_max = ( known after apply ) + source_port_min = ( known after apply ) } } } # module.ibm-vpc-subnets.ibm_is_network_acl_rule.acl_rule[0] will be created + resource \"ibm_is_network_acl_rule\" \"acl_rule\" { + action = \"allow\" + before = ( known after apply ) + destination = \"10.0.0.0/8\" + direction = \"inbound\" + href = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = \"default-vpc-subnet-default-allow-ingress-internal\" + network_acl = ( known after apply ) + protocol = ( known after apply ) + rule_id = ( known after apply ) + source = \"10.0.0.0/8\" } # module.ibm-vpc-subnets.ibm_is_network_acl_rule.acl_rule[1] will be created + resource \"ibm_is_network_acl_rule\" \"acl_rule\" { + action = \"allow\" + before = ( known after apply ) + destination = \"10.0.0.0/8\" + direction = \"inbound\" + href = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = \"default-vpc-subnet-default-allow-roks-ingress\" + network_acl = ( known after apply ) + protocol = ( known after apply ) + rule_id = ( known after apply ) + source = \"166.8.0.0/14\" } # module.ibm-vpc-subnets.ibm_is_network_acl_rule.acl_rule[2] will be created + resource \"ibm_is_network_acl_rule\" \"acl_rule\" { + action = \"allow\" + before = ( known after apply ) + destination = \"10.0.0.0/8\" + direction = \"inbound\" + href = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = \"default-vpc-subnet-default-allow-vse-ingress\" + network_acl = ( known after apply ) + protocol = ( known after apply ) + rule_id = ( known after apply ) + source = \"161.26.0.0/16\" } # module.ibm-vpc-subnets.ibm_is_network_acl_rule.acl_rule[3] will be created + resource \"ibm_is_network_acl_rule\" \"acl_rule\" { + action = \"allow\" + before = ( known after apply ) + destination = \"10.0.0.0/8\" + direction = \"outbound\" + href = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = \"default-vpc-subnet-default-allow-egress-internal\" + network_acl = ( known after apply ) + protocol = ( known after apply ) + rule_id = ( known after apply ) + source = \"10.0.0.0/8\" } # module.ibm-vpc-subnets.ibm_is_network_acl_rule.acl_rule[4] will be created + resource \"ibm_is_network_acl_rule\" \"acl_rule\" { + action = \"allow\" + before = ( known after apply ) + destination = \"166.8.0.0/14\" + direction = \"outbound\" + href = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = \"default-vpc-subnet-default-allow-roks-egress\" + network_acl = ( known after apply ) + protocol = ( known after apply ) + rule_id = ( known after apply ) + source = \"10.0.0.0/8\" } # module.ibm-vpc-subnets.ibm_is_network_acl_rule.acl_rule[5] will be created + resource \"ibm_is_network_acl_rule\" \"acl_rule\" { + action = \"allow\" + before = ( known after apply ) + destination = \"161.26.0.0/16\" + direction = \"outbound\" + href = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = \"default-vpc-subnet-default-allow-vse-egress\" + network_acl = ( known after apply ) + protocol = ( known after apply ) + rule_id = ( known after apply ) + source = \"10.0.0.0/8\" } # module.ibm-vpc-subnets.ibm_is_subnet.vpc_subnets[0] will be created + resource \"ibm_is_subnet\" \"vpc_subnets\" { + access_tags = ( known after apply ) + available_ipv4_address_count = ( known after apply ) + crn = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + ipv4_cidr_block = ( known after apply ) + name = \"default-vpc-subnet-default01\" + network_acl = ( known after apply ) + public_gateway = ( known after apply ) + resource_controller_url = ( known after apply ) + resource_crn = ( known after apply ) + resource_group = ( known after apply ) + resource_group_name = ( known after apply ) + resource_name = ( known after apply ) + resource_status = ( known after apply ) + routing_table = ( known after apply ) + status = ( known after apply ) + tags = ( known after apply ) + total_ipv4_address_count = 256 + vpc = ( known after apply ) + zone = \"us-south-1\" } # module.ibm-vpc-subnets.ibm_is_subnet.vpc_subnets[1] will be created + resource \"ibm_is_subnet\" \"vpc_subnets\" { + access_tags = ( known after apply ) + available_ipv4_address_count = ( known after apply ) + crn = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + ipv4_cidr_block = ( known after apply ) + name = \"default-vpc-subnet-default02\" + network_acl = ( known after apply ) + public_gateway = ( known after apply ) + resource_controller_url = ( known after apply ) + resource_crn = ( known after apply ) + resource_group = ( known after apply ) + resource_group_name = ( known after apply ) + resource_name = ( known after apply ) + resource_status = ( known after apply ) + routing_table = ( known after apply ) + status = ( known after apply ) + tags = ( known after apply ) + total_ipv4_address_count = 256 + vpc = ( known after apply ) + zone = \"us-south-2\" } # module.ibm-vpc-subnets.ibm_is_subnet.vpc_subnets[2] will be created + resource \"ibm_is_subnet\" \"vpc_subnets\" { + access_tags = ( known after apply ) + available_ipv4_address_count = ( known after apply ) + crn = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + ipv4_cidr_block = ( known after apply ) + name = \"default-vpc-subnet-default03\" + network_acl = ( known after apply ) + public_gateway = ( known after apply ) + resource_controller_url = ( known after apply ) + resource_crn = ( known after apply ) + resource_group = ( known after apply ) + resource_group_name = ( known after apply ) + resource_name = ( known after apply ) + resource_status = ( known after apply ) + routing_table = ( known after apply ) + status = ( known after apply ) + tags = ( known after apply ) + total_ipv4_address_count = 256 + vpc = ( known after apply ) + zone = \"us-south-3\" } # module.ibm-vpc-subnets.null_resource.print_names will be created + resource \"null_resource\" \"print_names\" { + id = ( known after apply ) } # module.resource_group.data.ibm_resource_group.resource_group will be read during apply # (config refers to values not yet known) < = data \"ibm_resource_group\" \"resource_group\" { + account_id = ( known after apply ) + created_at = ( known after apply ) + crn = ( known after apply ) + id = ( known after apply ) + is_default = ( known after apply ) + name = \"default\" + payment_methods_url = ( known after apply ) + quota_id = ( known after apply ) + quota_url = ( known after apply ) + resource_linkages = ( known after apply ) + state = ( known after apply ) + teams_url = ( known after apply ) + updated_at = ( known after apply ) } # module.resource_group.data.ibm_resource_tag.resource_group_tags will be read during apply # (config refers to values not yet known) < = data \"ibm_resource_tag\" \"resource_group_tags\" { + id = ( known after apply ) + resource_id = ( known after apply ) + tags = ( known after apply ) } # module.resource_group.null_resource.resource_group will be created + resource \"null_resource\" \"resource_group\" { + id = ( known after apply ) + triggers = ( known after apply ) } # module.resource_group.null_resource.wait_for_sync will be created + resource \"null_resource\" \"wait_for_sync\" { + id = ( known after apply ) } # module.resource_group.random_uuid.tag will be created + resource \"random_uuid\" \"tag\" { + id = ( known after apply ) + result = ( known after apply ) } # module.resource_group.module.clis.data.external.setup-binaries will be read during apply # (config refers to values not yet known) < = data \"external\" \"setup-binaries\" { + id = ( known after apply ) + program = [ + \"bash\" , + \".terraform/modules/resource_group.clis/scripts/setup-binaries.sh\" , ] + query = { + \"bin_dir\" = \"/Users/thomassuedbroecker/Downloads/dev/iascable-starting-point/examples/output/ibm-vpc/terraform/bin2\" + \"clis\" = \"yq,jq,igc\" + \"uuid\" = ( known after apply ) } + result = ( known after apply ) } # module.resource_group.module.clis.null_resource.print will be created + resource \"null_resource\" \"print\" { + id = ( known after apply ) } # module.resource_group.module.clis.random_string.uuid will be created + resource \"random_string\" \"uuid\" { + id = ( known after apply ) + length = 16 + lower = true + min_lower = 0 + min_numeric = 0 + min_special = 0 + min_upper = 0 + number = false + numeric = ( known after apply ) + result = ( known after apply ) + special = false + upper = false } Plan: 36 to add, 0 to change, 0 to destroy. \u2577 \u2502 Warning: Experimental feature \"module_variable_optional_attrs\" is active \u2502 \u2502 on .terraform/modules/ibm-vpc-subnets/version.tf line 10 , in terraform: \u2502 10 : experiments = [ module_variable_optional_attrs ] \u2502 \u2502 Experimental features are subject to breaking changes in \u2502 future minor or patch releases, based on feedback. \u2502 \u2502 If you have feedback on the design of this feature, please \u2502 open a GitHub issue to discuss it. \u2575 \u2577 \u2502 Warning: Argument is deprecated \u2502 \u2502 with module.resource_group.module.clis.random_string.uuid, \u2502 on .terraform/modules/resource_group.clis/main.tf line 15 , in resource \"random_string\" \"uuid\" : \u2502 15 : number = false \u2502 \u2502 Use numeric instead. \u2502 \u2502 ( and one more similar warning elsewhere ) Do you want to perform these actions? Terraform will perform the actions described above. Only 'yes' will be accepted to approve. Enter a value: yes Final result of the creation Apply complete! Resources: 36 added, 0 changed, 0 destroyed.","title":"Step 8: Execute the terraform apply"},{"location":"getting-started/lab1/#step-9-execute-the-terraform-destroy-command","text":"Note: Ensure you didn't delete the terraform.tfstate and the .terraform.lock.hcl files before. To destroy the provisioned resources, run the following: terraform destroy -auto-approve You need to ensure to provide the IBM Cloud API Key, the region and the resource group name. var.ibmcloud_api_key the value of ibmcloud_api_key Enter a value: xxxx var.region The IBM Cloud region where the cluster will be/has been installed. Enter a value: us-south var.resource_group_name The name of the resource group Enter a value: default Output overview: \u2577 \u2502 Warning: Experimental feature \"module_variable_optional_attrs\" is active \u2502 \u2502 on .terraform/modules/ibm-vpc-subnets/version.tf line 10 , in terraform: \u2502 10 : experiments = [ module_variable_optional_attrs ] \u2502 \u2502 Experimental features are subject to breaking changes in \u2502 future minor or patch releases, based on feedback. \u2502 \u2502 If you have feedback on the design of this feature, please \u2502 open a GitHub issue to discuss it. \u2575 \u2577 \u2502 Warning: Argument is deprecated \u2502 \u2502 with module.resource_group.module.clis.random_string.uuid, \u2502 on .terraform/modules/resource_group.clis/main.tf line 15 , in resource \"random_string\" \"uuid\" : \u2502 15 : number = false \u2502 \u2502 Use numeric instead. \u2502 \u2502 ( and one more similar warning elsewhere ) \u2575 Destroy complete! Resources: 36 destroyed.","title":"Step 9: Execute the terraform destroy command"},{"location":"getting-started/lab1/#4-summary","text":"The IasCable and the Modules (from the Software Everywhere project) which are all from the Cloud Native Toolkit providing a good concept for a framework to provide reusable components to install and configure cloud infrastructure. This was just a getting started. There is more to learn.","title":"4. Summary"},{"location":"getting-started/lab2/","text":"Lab 2: Use IasCable to create a VPC and a Red Hat OpenShift cluster on IBM Cloud \u00b6 That lab uses following resource as input: From IasCable and that blog post . The following list represents the modules which are referenced in the example IBM ROKS Bill of Materials for IasCable . IBM VPC ibm-vpc IBM VPC Subnets ibm-vpc-subnets IBM Cloud VPC Public Gateway ibm-vpc-gateways IBM OpenShift VPC cluster ibm-ocp-vpc 1. Pre-requisites for the example \u00b6 Following tools need to be installed on your local computer to follow the step by step instructions. Terraform Git That is the cloud environment we will use. IBM Cloud 2. Step-by-step example setup \u00b6 This is a step by step example setup to create a Virtual Private Cloud and an IBM Cloud managed Red Hat OpenShift cluster . 1 x Virtual Private Cloud 3 x Subnets 2 x Access Control Lists 1 x Routing Table 2 x Security Groups 3 x Public Gateway 1 x Virtual Private Endpoint Gateway 1 x Red Hat OpenShift cluster 3 x Worker Nodes one in each zone 1 x Default worker pool 1 x Cloud Object Storage Simplified architecture overview Step 1: Write the Bill of Material BOM file \u00b6 nano my-vpc-roks-bom.yaml Copy and past the following content into the my-vpc-roks-bom.yaml file. apiVersion : cloudnativetoolkit.dev/v1alpha1 kind : BillOfMaterial metadata : name : my-ibm-vpc-roks spec : modules : - name : ibm-vpc - name : ibm-vpc-subnets - name : ibm-vpc-gateways - name : ibm-ocp-vpc variables : - name : worker_count value : 1 Step 2: Build the project based on Bill of Material BOM file \u00b6 iascable build -i my-vpc-roks-bom.yaml Output: Loading catalog from url: https://modules.cloudnativetoolkit.dev/index.yaml Name: my-ibm-vpc-roks Writing output to: ./output Step 3: Verify the created files and folders \u00b6 tree Output: . \u251c\u2500\u2500 my-vpc-roks-bom.yaml \u2514\u2500\u2500 output \u251c\u2500\u2500 launch.sh \u2514\u2500\u2500 my-ibm-vpc-roks \u251c\u2500\u2500 apply.sh \u251c\u2500\u2500 bom.yaml \u251c\u2500\u2500 dependencies.dot \u251c\u2500\u2500 destroy.sh \u2514\u2500\u2500 terraform \u251c\u2500\u2500 docs \u2502 \u251c\u2500\u2500 ibm-object-storage.md \u2502 \u251c\u2500\u2500 ibm-ocp-vpc.md \u2502 \u251c\u2500\u2500 ibm-resource-group.md \u2502 \u251c\u2500\u2500 ibm-vpc-gateways.md \u2502 \u251c\u2500\u2500 ibm-vpc-subnets.md \u2502 \u2514\u2500\u2500 ibm-vpc.md \u251c\u2500\u2500 main.tf \u251c\u2500\u2500 my-ibm-vpc-roks.auto.tfvars \u251c\u2500\u2500 providers.tf \u251c\u2500\u2500 variables.tf \u2514\u2500\u2500 version.tf 4 directories, 17 files You can find details of the created files and folders also in IasCable starting point GitHub project and that blog post . In the newly created bom.yaml file we find more detailed information about modules we are going to use. apiVersion : cloudnativetoolkit.dev/v1alpha1 kind : BillOfMaterial metadata : name : my-ibm-vpc-roks spec : modules : - name : ibm-ocp-vpc alias : cluster version : v1.15.4 variables : - name : worker_count value : 1 - name : ibm-vpc alias : ibm-vpc version : v1.16.0 - name : ibm-vpc-gateways alias : ibm-vpc-gateways version : v1.9.0 - name : ibm-vpc-subnets alias : ibm-vpc-subnets version : v1.13.2 - name : ibm-resource-group alias : resource_group version : v3.2.16 - name : ibm-object-storage alias : cos version : v4.0.3 variables : - name : region type : string description : The IBM Cloud region where the cluster will be/has been installed. - name : ibmcloud_api_key type : string description : The IBM Cloud api token - name : worker_count type : number description : >- The number of worker nodes that should be provisioned for classic infrastructure defaultValue : 1 - name : cluster_flavor type : string description : The machine type that will be provisioned for classic infrastructure defaultValue : bx2.4x16 - name : ibm-vpc-subnets__count type : number description : The number of subnets that should be provisioned defaultValue : 3 - name : resource_group_name type : string description : The name of the resource group (Network) IBM VPC ibm-vpc (Network) IBM VPC Subnets ibm-vpc-subnets (Network) IBM Cloud VPC Public Gateway ibm-vpc-gateways This module makes use of the output from other modules: * Resource group - github.com/cloud-native-toolkit/terraform-ibm-resource-group.git * VPC - github.com/cloud-native-toolkit/terraform-ibm-vpc.git (Cluster) IBM OpenShift VPC cluster ibm-ocp-vpc This module makes use of the output from other modules: * Object Storage - github.com/cloud-native-toolkit/terraform-ibm-object-storage.git * VPC - github.com/cloud-native-toolkit/terraform-ibm-vpc.git * Subnet - github.com/cloud-native-toolkit/terraform-ibm-vpc.git Added modules based on module dependencies: (IAM) IBM Resource Group ibm-resource-group (Storage) IBM Object Storage ibm-object-storage We can verify the dependencies with the dependencies.dot content for example in Graphviz Online . Step 4: Execute the terraform init command \u00b6 Navigate to the output/my-ibm-vpc-roks/terraform folder and execute the terraform init command. cd output/my-ibm-vpc-roks/terraform terraform init Step 5: Execute the terraform apply command \u00b6 Execute the terraform apply command. terraform apply -auto-approve Note: You can create an IBM Cloud API Key with following command: ibmcloud iam api-key-create iascable-example . Input of your variables: var.ibmcloud_api_key The IBM Cloud api token Enter a value: XXX var.region The IBM Cloud region where the cluster will be/has been installed. Enter a value: eu-de var.resource_group_name The name of the resource group Enter a value: default Output: Finally you should get this output in your terminal. ... Apply complete! Resources: 56 added, 0 changed, 0 destroyed. Following files were created, that you should delete because these will used for the deletion or update of the resources. I added those files in my case to .gitignore . example/output/my-ibm-vpc-roks/terraform/.terraform.lock.hcl example/output/my-ibm-vpc-roks/terraform/clis-debug.log example/output/my-ibm-vpc-roks/terraform/.kube/b88790957171731697722dc07f0f923283278cf784b9fce792b831afbab8d83e_default-cluster_admin_k8sconfig/admin-key.pem example/output/my-ibm-vpc-roks/terraform/.kube/b88790957171731697722dc07f0f923283278cf784b9fce792b831afbab8d83e_default-cluster_admin_k8sconfig/admin.pem example/output/my-ibm-vpc-roks/terraform/.kube/b88790957171731697722dc07f0f923283278cf784b9fce792b831afbab8d83e_default-cluster_admin_k8sconfig/config.yml example/output/my-ibm-vpc-roks/terraform/.kube/b88790957171731697722dc07f0f923283278cf784b9fce792b831afbab8d83e_default-cluster_k8sconfig/config.yml example/output/my-ibm-vpc-roks/terraform/bin2/.igc-release example/output/my-ibm-vpc-roks/terraform/bin2/igc example/output/my-ibm-vpc-roks/terraform/bin2/jq example/output/my-ibm-vpc-roks/terraform/bin2/yq3 example/output/my-ibm-vpc-roks/terraform/bin2/yq4 Step 6: Execute the terraform destroy command \u00b6 Note: Ensure you didn't delete created files before. To destroy the provisioned resources, run the following: terraform destroy -auto-approve You need to provide the IBM Cloud API Key, the region and the resource group name again, because we didn't save those values in variables. var.ibmcloud_api_key The IBM Cloud api token Enter a value: XXX var.region The IBM Cloud region where the cluster will be/has been installed. Enter a value: eu-de var.resource_group_name The name of the resource group Enter a value: default Output: The final output should be: Destroy complete! Resources: 56 destroyed. 3. Summary \u00b6 When you use IasCable and the Modules it is much easier to setup infrastructure on a cloud using Terraform . The Modules containing a lot of pre-work and IasCable creates automated an awesome starting point for you. Surely you still should understand what are you creating and the final architecture you will be produce. Especially the pre-work for the example we use which divides the worker nodes over 3 zones, does pre-configurations of rules for the security groups and many more. So I looking forward now to get started with the GitOps topic which is also a part of the Modules in that framework.","title":"Lab 2 - Use IasCable to create a VPC and a Red Hat OpenShift cluster on IBM Cloud"},{"location":"getting-started/lab2/#lab-2-use-iascable-to-create-a-vpc-and-a-red-hat-openshift-cluster-on-ibm-cloud","text":"That lab uses following resource as input: From IasCable and that blog post . The following list represents the modules which are referenced in the example IBM ROKS Bill of Materials for IasCable . IBM VPC ibm-vpc IBM VPC Subnets ibm-vpc-subnets IBM Cloud VPC Public Gateway ibm-vpc-gateways IBM OpenShift VPC cluster ibm-ocp-vpc","title":"Lab 2: Use IasCable to create a VPC and a Red Hat OpenShift cluster on IBM Cloud"},{"location":"getting-started/lab2/#1-pre-requisites-for-the-example","text":"Following tools need to be installed on your local computer to follow the step by step instructions. Terraform Git That is the cloud environment we will use. IBM Cloud","title":"1. Pre-requisites for the example"},{"location":"getting-started/lab2/#2-step-by-step-example-setup","text":"This is a step by step example setup to create a Virtual Private Cloud and an IBM Cloud managed Red Hat OpenShift cluster . 1 x Virtual Private Cloud 3 x Subnets 2 x Access Control Lists 1 x Routing Table 2 x Security Groups 3 x Public Gateway 1 x Virtual Private Endpoint Gateway 1 x Red Hat OpenShift cluster 3 x Worker Nodes one in each zone 1 x Default worker pool 1 x Cloud Object Storage Simplified architecture overview","title":"2. Step-by-step example setup"},{"location":"getting-started/lab2/#step-1-write-the-bill-of-material-bom-file","text":"nano my-vpc-roks-bom.yaml Copy and past the following content into the my-vpc-roks-bom.yaml file. apiVersion : cloudnativetoolkit.dev/v1alpha1 kind : BillOfMaterial metadata : name : my-ibm-vpc-roks spec : modules : - name : ibm-vpc - name : ibm-vpc-subnets - name : ibm-vpc-gateways - name : ibm-ocp-vpc variables : - name : worker_count value : 1","title":"Step 1: Write the Bill of Material BOM file"},{"location":"getting-started/lab2/#step-2-build-the-project-based-on-bill-of-material-bom-file","text":"iascable build -i my-vpc-roks-bom.yaml Output: Loading catalog from url: https://modules.cloudnativetoolkit.dev/index.yaml Name: my-ibm-vpc-roks Writing output to: ./output","title":"Step 2: Build the project based on Bill of Material BOM file"},{"location":"getting-started/lab2/#step-3-verify-the-created-files-and-folders","text":"tree Output: . \u251c\u2500\u2500 my-vpc-roks-bom.yaml \u2514\u2500\u2500 output \u251c\u2500\u2500 launch.sh \u2514\u2500\u2500 my-ibm-vpc-roks \u251c\u2500\u2500 apply.sh \u251c\u2500\u2500 bom.yaml \u251c\u2500\u2500 dependencies.dot \u251c\u2500\u2500 destroy.sh \u2514\u2500\u2500 terraform \u251c\u2500\u2500 docs \u2502 \u251c\u2500\u2500 ibm-object-storage.md \u2502 \u251c\u2500\u2500 ibm-ocp-vpc.md \u2502 \u251c\u2500\u2500 ibm-resource-group.md \u2502 \u251c\u2500\u2500 ibm-vpc-gateways.md \u2502 \u251c\u2500\u2500 ibm-vpc-subnets.md \u2502 \u2514\u2500\u2500 ibm-vpc.md \u251c\u2500\u2500 main.tf \u251c\u2500\u2500 my-ibm-vpc-roks.auto.tfvars \u251c\u2500\u2500 providers.tf \u251c\u2500\u2500 variables.tf \u2514\u2500\u2500 version.tf 4 directories, 17 files You can find details of the created files and folders also in IasCable starting point GitHub project and that blog post . In the newly created bom.yaml file we find more detailed information about modules we are going to use. apiVersion : cloudnativetoolkit.dev/v1alpha1 kind : BillOfMaterial metadata : name : my-ibm-vpc-roks spec : modules : - name : ibm-ocp-vpc alias : cluster version : v1.15.4 variables : - name : worker_count value : 1 - name : ibm-vpc alias : ibm-vpc version : v1.16.0 - name : ibm-vpc-gateways alias : ibm-vpc-gateways version : v1.9.0 - name : ibm-vpc-subnets alias : ibm-vpc-subnets version : v1.13.2 - name : ibm-resource-group alias : resource_group version : v3.2.16 - name : ibm-object-storage alias : cos version : v4.0.3 variables : - name : region type : string description : The IBM Cloud region where the cluster will be/has been installed. - name : ibmcloud_api_key type : string description : The IBM Cloud api token - name : worker_count type : number description : >- The number of worker nodes that should be provisioned for classic infrastructure defaultValue : 1 - name : cluster_flavor type : string description : The machine type that will be provisioned for classic infrastructure defaultValue : bx2.4x16 - name : ibm-vpc-subnets__count type : number description : The number of subnets that should be provisioned defaultValue : 3 - name : resource_group_name type : string description : The name of the resource group (Network) IBM VPC ibm-vpc (Network) IBM VPC Subnets ibm-vpc-subnets (Network) IBM Cloud VPC Public Gateway ibm-vpc-gateways This module makes use of the output from other modules: * Resource group - github.com/cloud-native-toolkit/terraform-ibm-resource-group.git * VPC - github.com/cloud-native-toolkit/terraform-ibm-vpc.git (Cluster) IBM OpenShift VPC cluster ibm-ocp-vpc This module makes use of the output from other modules: * Object Storage - github.com/cloud-native-toolkit/terraform-ibm-object-storage.git * VPC - github.com/cloud-native-toolkit/terraform-ibm-vpc.git * Subnet - github.com/cloud-native-toolkit/terraform-ibm-vpc.git Added modules based on module dependencies: (IAM) IBM Resource Group ibm-resource-group (Storage) IBM Object Storage ibm-object-storage We can verify the dependencies with the dependencies.dot content for example in Graphviz Online .","title":"Step 3: Verify the created files and folders"},{"location":"getting-started/lab2/#step-4-execute-the-terraform-init-command","text":"Navigate to the output/my-ibm-vpc-roks/terraform folder and execute the terraform init command. cd output/my-ibm-vpc-roks/terraform terraform init","title":"Step 4: Execute the terraform init command"},{"location":"getting-started/lab2/#step-5-execute-the-terraform-apply-command","text":"Execute the terraform apply command. terraform apply -auto-approve Note: You can create an IBM Cloud API Key with following command: ibmcloud iam api-key-create iascable-example . Input of your variables: var.ibmcloud_api_key The IBM Cloud api token Enter a value: XXX var.region The IBM Cloud region where the cluster will be/has been installed. Enter a value: eu-de var.resource_group_name The name of the resource group Enter a value: default Output: Finally you should get this output in your terminal. ... Apply complete! Resources: 56 added, 0 changed, 0 destroyed. Following files were created, that you should delete because these will used for the deletion or update of the resources. I added those files in my case to .gitignore . example/output/my-ibm-vpc-roks/terraform/.terraform.lock.hcl example/output/my-ibm-vpc-roks/terraform/clis-debug.log example/output/my-ibm-vpc-roks/terraform/.kube/b88790957171731697722dc07f0f923283278cf784b9fce792b831afbab8d83e_default-cluster_admin_k8sconfig/admin-key.pem example/output/my-ibm-vpc-roks/terraform/.kube/b88790957171731697722dc07f0f923283278cf784b9fce792b831afbab8d83e_default-cluster_admin_k8sconfig/admin.pem example/output/my-ibm-vpc-roks/terraform/.kube/b88790957171731697722dc07f0f923283278cf784b9fce792b831afbab8d83e_default-cluster_admin_k8sconfig/config.yml example/output/my-ibm-vpc-roks/terraform/.kube/b88790957171731697722dc07f0f923283278cf784b9fce792b831afbab8d83e_default-cluster_k8sconfig/config.yml example/output/my-ibm-vpc-roks/terraform/bin2/.igc-release example/output/my-ibm-vpc-roks/terraform/bin2/igc example/output/my-ibm-vpc-roks/terraform/bin2/jq example/output/my-ibm-vpc-roks/terraform/bin2/yq3 example/output/my-ibm-vpc-roks/terraform/bin2/yq4","title":"Step 5: Execute the terraform apply  command"},{"location":"getting-started/lab2/#step-6-execute-the-terraform-destroy-command","text":"Note: Ensure you didn't delete created files before. To destroy the provisioned resources, run the following: terraform destroy -auto-approve You need to provide the IBM Cloud API Key, the region and the resource group name again, because we didn't save those values in variables. var.ibmcloud_api_key The IBM Cloud api token Enter a value: XXX var.region The IBM Cloud region where the cluster will be/has been installed. Enter a value: eu-de var.resource_group_name The name of the resource group Enter a value: default Output: The final output should be: Destroy complete! Resources: 56 destroyed.","title":"Step 6: Execute the terraform destroy command"},{"location":"getting-started/lab2/#3-summary","text":"When you use IasCable and the Modules it is much easier to setup infrastructure on a cloud using Terraform . The Modules containing a lot of pre-work and IasCable creates automated an awesome starting point for you. Surely you still should understand what are you creating and the final architecture you will be produce. Especially the pre-work for the example we use which divides the worker nodes over 3 zones, does pre-configurations of rules for the security groups and many more. So I looking forward now to get started with the GitOps topic which is also a part of the Modules in that framework.","title":"3. Summary"},{"location":"getting-started/lab3/","text":"Lab 3: Use Software Everywhere and IasCable to setup GitOps on a Red Hat OpenShift Cluster in a Virtual Private Cloud on IBM Cloud \u00b6 That lab uses following resource as input: From IasCable and that blog post . Our objective is to create a customized initial GitOps setup in an IBM Cloud environment. The Software Everywhere project and IasCable CLI do provide an awesome way to eliminate writing Terraform modules for various clouds such as IBM Cloud , AWS or Azure to create and configure resources. We are going to reuse Terraform modules which the Software Everywhere catalog does provide. Surely, we need to know the needed outline for the cloud architecture which does depend on the cloud environment we are going to use. As I said the Software Everywhere catalog does provide the reuse of existing Terraform modules, which we use by just combining by writing a \" Bill of Material file \" and configure the variables for the related Terraform modules ( example link to the GitOps Terraform module ) when it is needed. We will not write any Terraform code , we will only combine existing Terraform modules and configure them using IasCable BOM files! In that scenario we will use IBM Cloud with a Virtual Private Cloud and a Red Hat OpenShift cluster with Argo CD installed and integrated with a GitHub project. These are the major sections: Define an outline of the target architecture Identify the needed Software Everywhere Terraform modules for the target architecture Write a customized BOM to combine the modules Use IasCable to create the scaffolding for a Terraform project Use the IasCable tools container to execute the Terraform modules Note: Depending on the container engine you are going to use on your computer, you maybe have to copy the Terraform project inside the running tools container, because of access right restrictions to access mapped local volumes to the running containers. That is the reason why I wrote some helper scripts to simplify the copy and deletion of the Terraform code mapped to the local volume of our computer. You can find the in the current helper bash automation script in the GitHub project . IasCable does suggest to use Docker or Colima Apply the Terraform modules to create the environment in IBM Cloud and backup the Terraform state to the local computer. Destroy the environment on IBM Cloud. Summary You can the access source code in the GitHub project I created. The project is under Apache-2.0 license . git clone https://github.com/thomassuedbroecker/iascable-vpc-openshift-argocd.git cd example 1. Define an outline of the target architecture \u00b6 This is our simplified target architecture for our objective to create a customized setup in an IBM Cloud environment for GitOps. Configuration of GitOps in Red Hat OpenShift We will use two operators: * Red at OpenShift GitOps operator * We will create one [ArgoCD instance](https://argo-cd.readthedocs.io/en/stable/) with the [Red at OpenShift GitOps operator](https://github.com/redhat-developer/gitops-operator), that [ArgoCD instance](https://argo-cd.readthedocs.io/en/stable/) will bin initial configured by a newly created GitHub project configure by a [Cloud Native Toolkit template](https://github.com/cloud-native-toolkit/terraform-tools-gitops/tree/main/template) for GitOps repositories. Red at OpenShift Pipelines operator . There will be no initial setup for a Tekton pipeline at the moment. IBM Cloud infrastructure with Red Hat OpenShift in a Virtual Private Cloud 2. Identify the needed Software Everywhere Terraform modules for the target architecture \u00b6 Let us first define which Software Everywhere Terraform modules we are going to use for our custom BOM file specification. The Software Everywhere project points to the Automated Solutions project which contains several starting points for various setups, which can be used as a starting point. In our case we have two major areas for Terraform modules we want to use: Configuration of GitOps IBM Cloud infrastructure 1. Configuration of GitOps \u00b6 IBM OpenShift login ocp-login - login to existing OpenShift cluster GitOps repo gitops-repo - creates the GitOps Repo ArgoCD Bootstrap argocd-bootstrap 2. Cloud infrastructure/services resources related \u00b6 IBM VPC ibm-vpc IBM VPC Subnets ibm-vpc-subnets IBM Cloud VPC Public Gateway ibm-vpc-gateways IBM OpenShift VPC cluster ibm-ocp-vpc IBM Object Storage ibm-object-storage 3. Write a customized BOM to combine the modules \u00b6 Step 1: Write the Bill of Material BOM file \u00b6 Now we combine the existing Terraform modules we got from the Software Everywhere catalog and we specify the variables in the BOM file we need to reflect our target architecture. Note: When we going to use these variables, we must keep in mind that we need to use the names of the variables defined in the Terraform modules and we should use alias: ibm-vpc to define the prefix in the BOM file. The BOM file for our architecture is divided in 3 main sections. Virtual Private Cloud Red Hat OpenShift Cluster (ROKS) GitOps We need to create an IBM Cloud API key and an Personal Access Token for the GitHub account . apiVersion : cloudnativetoolkit.dev/v1alpha1 kind : BillOfMaterial metadata : name : my-ibm-vpc-roks-argocd spec : modules : # Virtual Private Cloud - related # - subnets # - gateways - name : ibm-vpc alias : ibm-vpc version : v1.16.0 variables : - name : name value : \"tsued-gitops-sample\" - name : tags value : [ \"tsuedro\" ] - name : ibm-vpc-subnets alias : ibm-vpc-subnets version : v1.13.2 variables : - name : _count value : 1 - name : name value : \"tsued-gitops-sample\" - name : tags value : [ \"tsuedro\" ] - name : ibm-vpc-gateways # ROKS - related # - objectstorage - name : ibm-ocp-vpc alias : ibm-ocp-vpc version : v1.15.5 variables : - name : name value : \"tsued-gitops\" - name : worker_count value : 2 - name : tags value : [ \"tsuedro\" ] - name : ibm-object-storage alias : ibm-object-storage version : v4.0.3 variables : - name : name value : \"cos_tsued_gitops\" - name : tags value : [ \"tsuedro\" ] - name : label value : [ \"cos_tsued\" ] # Install OpenShift GitOps and Bootstrap GitOps (aka. ArgoCD) - related # - argocd # - gitops - name : argocd-bootstrap alias : argocd-bootstrap version : v1.12.0 variables : - name : repo_token - name : gitops-repo alias : gitops-repo version : v1.20.2 variables : - name : host value : \"github.com\" - name : type value : \"GIT\" - name : org value : \"thomassuedbroecker\" - name : username value : \"thomassuedbroecker\" - name : project value : \"iascable-gitops\" - name : repo value : \"iascable-gitops\" The BOM will result later in following overall dependencies for the used Terraform modules after the usage of IasCable . The dependencies are given in automatic created dependencies.dot file later. Note: You can use GraphvizOnline for the visualization. 4. Use IasCable to create the scaffolding for a Terraform project \u00b6 Step 1: Install colima container engine and start the container engine \u00b6 Example for an installation of colima on macOS. brew install docker colima colima start Step 2: Create a terraform project based on Bill of Material BOM file \u00b6 Version iascable --version Output: 2 .14.1 Build iascable build -i my-vpc-roks-argocd-bom.yaml Output: Loading catalog from url: https://modules.cloudnativetoolkit.dev/index.yaml Name: my-ibm-vpc-roks-argocd Writing output to: ./output Step 3: Copy helper bash scripts into the output folder \u00b6 cp helper-tools-create-container-workspace.sh ./output cp helper-tools-execute-apply-and-backup-result.sh ./output cp helper-tools-execute-destroy-and-delete-backup.sh ./output Step 4: Start the tools container provided by the IasCable \u00b6 Note: At the moment we need to change and save the launch.sh script a bit. Open the launch.sh script. cd output nano launch.sh Delete the -u \"${UID}\" parameter Before ${ DOCKER_CMD } run -itd --name ${ CONTAINER_NAME } -u \" ${ UID } \" -v \" ${ SRC_DIR } :/terraform\" -v \"workspace- ${ AUTOMATION_BASE } :/workspaces\" ${ ENV_FILE } -w /terraform ${ DOCKER_IMAGE } After the change ${ DOCKER_CMD } run -itd --name ${ CONTAINER_NAME } -v \" ${ SRC_DIR } :/terraform\" -v \"workspace- ${ AUTOMATION_BASE } :/workspaces\" ${ ENV_FILE } -w /terraform ${ DOCKER_IMAGE } Execute the launch.sh script sh launch.sh 5. Use the IasCable tools container to execute the Terraform modules \u00b6 Step 1 (inside the container): In the running container verify the mapped resources \u00b6 ~/src $ ls helper-tools-create-container-workspace.sh helper-tools-execute-apply-and-backup-result.sh helper-tools-execute-destroy-and-delete-backup.sh launch.sh my-ibm-vpc-roks-argocd Step 2 (inside the container): Create a workspace folder in your container and copy your IasCable project into it \u00b6 sh helper-tools-create-container-workspace.sh ls /home/devops/workspace The following tasks are automated in the helper bash script helper-tools-create-container-workspace.sh I wrote. Creates a workspace folder Copies the Terraform project from the mapped volume folder to the workspace folder Output: You can see the copied Terraform project folder inside the container. my-ibm-vpc-roks-argocd 6. Apply the Terraform modules to create the environment in IBM Cloud and backup Terraform configuration \u00b6 Step 1 (inside the container): Execute the apply.sh and backup the result into the mapped volume \u00b6 All these tasks are automated in the helper bash script I wrote. sh helper-tools-execute-apply-and-backup-result.sh The script helper-tools-execute-apply-and-backup-result.sh does following: Navigate to the created workspace Execute apply.sh script List the created resources Copy current start to mapped volume Interactive output: As we see in the output the values we inserted in our custom BOM file are now used as the default values. In our example we only need to insert the values for: gitops-repo_token ibmcloud_api_key resource_group_name region Variables can be provided in a yaml file passed as the first argument Provide a value for 'gitops-repo_host' : The host for the git repository. The git host used can be a GitHub, GitHub Enterprise, Gitlab, Bitbucket, Gitea or Azure DevOps server. If the host is null assumes in -cluster Gitea instance will be used. > ( github.com ) Provide a value for 'gitops-repo_org' : The org/group where the git repository exists/will be provisioned. If the value is left blank then the username org will be used. > ( thomassuedbroecker ) Provide a value for 'gitops-repo_project' : The project that will be used for the git repo. ( Primarily used for Azure DevOps repos ) > ( iascable-gitops ) Provide a value for 'gitops-repo_username' : The username of the user with access to the repository > ( thomassuedbroecker ) Provide a value for 'gitops-repo_token' : The personal access token used to access the repository > XXX > Provide a value for 'ibmcloud_api_key' : > XXX Provide a value for 'region' : > eu-de Provide a value for 'worker_count' : The number of worker nodes that should be provisioned for classic infrastructure > ( 2 ) Provide a value for 'ibm-ocp-vpc_flavor' : The machine type that will be provisioned for classic infrastructure > ( bx2.4x16 ) Provide a value for 'ibm-vpc-subnets__count' : The number of subnets that should be provisioned > ( 1 ) Provide a value for 'resource_group_name' : The name of the resource group > default Output: Move on with the setup and apply Terraform. Do you want to perform these actions? Terraform will perform the actions described above. Only 'yes' will be accepted to approve. Enter a value: yes After a while you should get following output. Apply complete! Resources: 91 added, 0 changed, 0 destroyed. Major resources which were created: Cloud infrastructure/services resources 1 x VPC 1 x Subnet 4 Security groups Two were created during the subnet creation and two are related to the created Red Hat OpenShift cluster. 1 x Virtual Private Endpoint 1 x Public Gateway 2 x Access Control Lists One was created for the VPC module and one during the creation by the subnet module. 1 x Routing Table 1 x Red Hat OpenShift Cluster 1 x Object Storage Cluster and GitOps configuration Red Hat OpenShift GitOps operator and Red Hat OpenShift Pipelines operator GitHub project as ArgoCD repository Preconfigure ArgoCD project The invoked apply.sh script will create following files or folders: Inside the tools container: a temporary workspace/my-ibm-vpc-roks-argocd/variables.yaml.tmp file a workspace/my-ibm-vpc-roks-argocd/variables.yaml file a workspace/my-ibm-vpc-roks-argocd/terraform/variables.tf file a workspace/my-ibm-vpc-roks-argocd/terraform/variables.tfvars file several folders .kube , .terraform , .tmp , bin2 , docs Then it creates a terraform.tfvars file based on the entries you gave and executes init and apply command from Terraform. Be aware that the IBM Cloud access key information and GitHub access token are saved in text format in the output/my-ibm-vpc-roks-argocd/terraform/terraform.tfvars file! Don't share this in a public GitHub repository. On GitHub it creates a GitHub private project which contains preconfigure ArgoCD resource provided by cloud native toolkit Note: Here you can sample of the content of an example for a generated variables.yaml file link and here you can find an example for the created BOM file . 7. Destroy the environment on IBM Cloud \u00b6 Step 1 (inside the container): Destroy the created IBM Cloud resources \u00b6 All these tasks are automated in the helper bash script I wrote. Note: Ensure you didn't delete created Terraform files before. sh helper-tools-execute-destroy-and-delete-backup.sh * The script helper-tools-execute-destroy-and-delete-backup.sh does following: Navigate to workspace Execute destroy.sh Navigate to the mapped volume Copy the current state to the mapped volume Output: Note: It also deleted the automated created private GitHub project. Destroy complete! Resources: 91 destroyed. 8. Summary \u00b6 We achieved what we wanted to achieve, create a customized initial setup in an IBM Cloud environment for GitOps . The Software Everywhere project and IasCable are powerful. As we have seen there was no need to write any Terraform module! Yes, when you are going to define you own \"Bill of Material BOM file\" you need to get familiar with the related modules related to your target architecture, when you want to customize it to your needs. But, as I said: There was no need to write own Terraform modules in our case. The Software Everywhere project and IasCable project needs some more documentation in the future, I like the power of it and it is under Apache-2.0 license , which means you can use it as your starting point for Software Everywhere with Terraform and contribute to the project.","title":"Lab 3 - Setup initial GitOps on a Red Hat OpenShift Cluster in a Virtual Private Cloud on IBM Cloud"},{"location":"getting-started/lab3/#lab-3-use-software-everywhere-and-iascable-to-setup-gitops-on-a-red-hat-openshift-cluster-in-a-virtual-private-cloud-on-ibm-cloud","text":"That lab uses following resource as input: From IasCable and that blog post . Our objective is to create a customized initial GitOps setup in an IBM Cloud environment. The Software Everywhere project and IasCable CLI do provide an awesome way to eliminate writing Terraform modules for various clouds such as IBM Cloud , AWS or Azure to create and configure resources. We are going to reuse Terraform modules which the Software Everywhere catalog does provide. Surely, we need to know the needed outline for the cloud architecture which does depend on the cloud environment we are going to use. As I said the Software Everywhere catalog does provide the reuse of existing Terraform modules, which we use by just combining by writing a \" Bill of Material file \" and configure the variables for the related Terraform modules ( example link to the GitOps Terraform module ) when it is needed. We will not write any Terraform code , we will only combine existing Terraform modules and configure them using IasCable BOM files! In that scenario we will use IBM Cloud with a Virtual Private Cloud and a Red Hat OpenShift cluster with Argo CD installed and integrated with a GitHub project. These are the major sections: Define an outline of the target architecture Identify the needed Software Everywhere Terraform modules for the target architecture Write a customized BOM to combine the modules Use IasCable to create the scaffolding for a Terraform project Use the IasCable tools container to execute the Terraform modules Note: Depending on the container engine you are going to use on your computer, you maybe have to copy the Terraform project inside the running tools container, because of access right restrictions to access mapped local volumes to the running containers. That is the reason why I wrote some helper scripts to simplify the copy and deletion of the Terraform code mapped to the local volume of our computer. You can find the in the current helper bash automation script in the GitHub project . IasCable does suggest to use Docker or Colima Apply the Terraform modules to create the environment in IBM Cloud and backup the Terraform state to the local computer. Destroy the environment on IBM Cloud. Summary You can the access source code in the GitHub project I created. The project is under Apache-2.0 license . git clone https://github.com/thomassuedbroecker/iascable-vpc-openshift-argocd.git cd example","title":"Lab 3: Use Software Everywhere and IasCable to setup GitOps on a Red Hat OpenShift Cluster in a Virtual Private Cloud on IBM Cloud"},{"location":"getting-started/lab3/#1-define-an-outline-of-the-target-architecture","text":"This is our simplified target architecture for our objective to create a customized setup in an IBM Cloud environment for GitOps. Configuration of GitOps in Red Hat OpenShift We will use two operators: * Red at OpenShift GitOps operator * We will create one [ArgoCD instance](https://argo-cd.readthedocs.io/en/stable/) with the [Red at OpenShift GitOps operator](https://github.com/redhat-developer/gitops-operator), that [ArgoCD instance](https://argo-cd.readthedocs.io/en/stable/) will bin initial configured by a newly created GitHub project configure by a [Cloud Native Toolkit template](https://github.com/cloud-native-toolkit/terraform-tools-gitops/tree/main/template) for GitOps repositories. Red at OpenShift Pipelines operator . There will be no initial setup for a Tekton pipeline at the moment. IBM Cloud infrastructure with Red Hat OpenShift in a Virtual Private Cloud","title":"1. Define an outline of the target architecture"},{"location":"getting-started/lab3/#2-identify-the-needed-software-everywhere-terraform-modules-for-the-target-architecture","text":"Let us first define which Software Everywhere Terraform modules we are going to use for our custom BOM file specification. The Software Everywhere project points to the Automated Solutions project which contains several starting points for various setups, which can be used as a starting point. In our case we have two major areas for Terraform modules we want to use: Configuration of GitOps IBM Cloud infrastructure","title":"2. Identify the needed Software Everywhere Terraform modules for the target architecture"},{"location":"getting-started/lab3/#1-configuration-of-gitops","text":"IBM OpenShift login ocp-login - login to existing OpenShift cluster GitOps repo gitops-repo - creates the GitOps Repo ArgoCD Bootstrap argocd-bootstrap","title":"1. Configuration of GitOps"},{"location":"getting-started/lab3/#2-cloud-infrastructureservices-resources-related","text":"IBM VPC ibm-vpc IBM VPC Subnets ibm-vpc-subnets IBM Cloud VPC Public Gateway ibm-vpc-gateways IBM OpenShift VPC cluster ibm-ocp-vpc IBM Object Storage ibm-object-storage","title":"2. Cloud infrastructure/services resources related"},{"location":"getting-started/lab3/#3-write-a-customized-bom-to-combine-the-modules","text":"","title":"3. Write a customized BOM to combine the modules"},{"location":"getting-started/lab3/#step-1-write-the-bill-of-material-bom-file","text":"Now we combine the existing Terraform modules we got from the Software Everywhere catalog and we specify the variables in the BOM file we need to reflect our target architecture. Note: When we going to use these variables, we must keep in mind that we need to use the names of the variables defined in the Terraform modules and we should use alias: ibm-vpc to define the prefix in the BOM file. The BOM file for our architecture is divided in 3 main sections. Virtual Private Cloud Red Hat OpenShift Cluster (ROKS) GitOps We need to create an IBM Cloud API key and an Personal Access Token for the GitHub account . apiVersion : cloudnativetoolkit.dev/v1alpha1 kind : BillOfMaterial metadata : name : my-ibm-vpc-roks-argocd spec : modules : # Virtual Private Cloud - related # - subnets # - gateways - name : ibm-vpc alias : ibm-vpc version : v1.16.0 variables : - name : name value : \"tsued-gitops-sample\" - name : tags value : [ \"tsuedro\" ] - name : ibm-vpc-subnets alias : ibm-vpc-subnets version : v1.13.2 variables : - name : _count value : 1 - name : name value : \"tsued-gitops-sample\" - name : tags value : [ \"tsuedro\" ] - name : ibm-vpc-gateways # ROKS - related # - objectstorage - name : ibm-ocp-vpc alias : ibm-ocp-vpc version : v1.15.5 variables : - name : name value : \"tsued-gitops\" - name : worker_count value : 2 - name : tags value : [ \"tsuedro\" ] - name : ibm-object-storage alias : ibm-object-storage version : v4.0.3 variables : - name : name value : \"cos_tsued_gitops\" - name : tags value : [ \"tsuedro\" ] - name : label value : [ \"cos_tsued\" ] # Install OpenShift GitOps and Bootstrap GitOps (aka. ArgoCD) - related # - argocd # - gitops - name : argocd-bootstrap alias : argocd-bootstrap version : v1.12.0 variables : - name : repo_token - name : gitops-repo alias : gitops-repo version : v1.20.2 variables : - name : host value : \"github.com\" - name : type value : \"GIT\" - name : org value : \"thomassuedbroecker\" - name : username value : \"thomassuedbroecker\" - name : project value : \"iascable-gitops\" - name : repo value : \"iascable-gitops\" The BOM will result later in following overall dependencies for the used Terraform modules after the usage of IasCable . The dependencies are given in automatic created dependencies.dot file later. Note: You can use GraphvizOnline for the visualization.","title":"Step 1: Write the Bill of Material BOM file"},{"location":"getting-started/lab3/#4-use-iascable-to-create-the-scaffolding-for-a-terraform-project","text":"","title":"4. Use IasCable to create the scaffolding for a Terraform project"},{"location":"getting-started/lab3/#step-1-install-colima-container-engine-and-start-the-container-engine","text":"Example for an installation of colima on macOS. brew install docker colima colima start","title":"Step 1: Install colima container engine and start the container engine"},{"location":"getting-started/lab3/#step-2-create-a-terraform-project-based-on-bill-of-material-bom-file","text":"Version iascable --version Output: 2 .14.1 Build iascable build -i my-vpc-roks-argocd-bom.yaml Output: Loading catalog from url: https://modules.cloudnativetoolkit.dev/index.yaml Name: my-ibm-vpc-roks-argocd Writing output to: ./output","title":"Step 2: Create a terraform project based on Bill of Material BOM file"},{"location":"getting-started/lab3/#step-3-copy-helper-bash-scripts-into-the-output-folder","text":"cp helper-tools-create-container-workspace.sh ./output cp helper-tools-execute-apply-and-backup-result.sh ./output cp helper-tools-execute-destroy-and-delete-backup.sh ./output","title":"Step 3: Copy helper bash scripts into the output folder"},{"location":"getting-started/lab3/#step-4-start-the-tools-container-provided-by-the-iascable","text":"Note: At the moment we need to change and save the launch.sh script a bit. Open the launch.sh script. cd output nano launch.sh Delete the -u \"${UID}\" parameter Before ${ DOCKER_CMD } run -itd --name ${ CONTAINER_NAME } -u \" ${ UID } \" -v \" ${ SRC_DIR } :/terraform\" -v \"workspace- ${ AUTOMATION_BASE } :/workspaces\" ${ ENV_FILE } -w /terraform ${ DOCKER_IMAGE } After the change ${ DOCKER_CMD } run -itd --name ${ CONTAINER_NAME } -v \" ${ SRC_DIR } :/terraform\" -v \"workspace- ${ AUTOMATION_BASE } :/workspaces\" ${ ENV_FILE } -w /terraform ${ DOCKER_IMAGE } Execute the launch.sh script sh launch.sh","title":"Step 4: Start the tools container provided by the IasCable"},{"location":"getting-started/lab3/#5-use-the-iascable-tools-container-to-execute-the-terraform-modules","text":"","title":"5. Use the IasCable tools container to execute the Terraform modules"},{"location":"getting-started/lab3/#step-1-inside-the-container-in-the-running-container-verify-the-mapped-resources","text":"~/src $ ls helper-tools-create-container-workspace.sh helper-tools-execute-apply-and-backup-result.sh helper-tools-execute-destroy-and-delete-backup.sh launch.sh my-ibm-vpc-roks-argocd","title":"Step 1 (inside the container): In the running container verify the mapped resources"},{"location":"getting-started/lab3/#step-2-inside-the-container-create-a-workspace-folder-in-your-container-and-copy-your-iascable-project-into-it","text":"sh helper-tools-create-container-workspace.sh ls /home/devops/workspace The following tasks are automated in the helper bash script helper-tools-create-container-workspace.sh I wrote. Creates a workspace folder Copies the Terraform project from the mapped volume folder to the workspace folder Output: You can see the copied Terraform project folder inside the container. my-ibm-vpc-roks-argocd","title":"Step 2 (inside the container): Create a workspace folder in your container and copy your IasCable project into it"},{"location":"getting-started/lab3/#6-apply-the-terraform-modules-to-create-the-environment-in-ibm-cloud-and-backup-terraform-configuration","text":"","title":"6. Apply the Terraform modules to create the environment in IBM Cloud and backup Terraform configuration"},{"location":"getting-started/lab3/#step-1-inside-the-container-execute-the-applysh-and-backup-the-result-into-the-mapped-volume","text":"All these tasks are automated in the helper bash script I wrote. sh helper-tools-execute-apply-and-backup-result.sh The script helper-tools-execute-apply-and-backup-result.sh does following: Navigate to the created workspace Execute apply.sh script List the created resources Copy current start to mapped volume Interactive output: As we see in the output the values we inserted in our custom BOM file are now used as the default values. In our example we only need to insert the values for: gitops-repo_token ibmcloud_api_key resource_group_name region Variables can be provided in a yaml file passed as the first argument Provide a value for 'gitops-repo_host' : The host for the git repository. The git host used can be a GitHub, GitHub Enterprise, Gitlab, Bitbucket, Gitea or Azure DevOps server. If the host is null assumes in -cluster Gitea instance will be used. > ( github.com ) Provide a value for 'gitops-repo_org' : The org/group where the git repository exists/will be provisioned. If the value is left blank then the username org will be used. > ( thomassuedbroecker ) Provide a value for 'gitops-repo_project' : The project that will be used for the git repo. ( Primarily used for Azure DevOps repos ) > ( iascable-gitops ) Provide a value for 'gitops-repo_username' : The username of the user with access to the repository > ( thomassuedbroecker ) Provide a value for 'gitops-repo_token' : The personal access token used to access the repository > XXX > Provide a value for 'ibmcloud_api_key' : > XXX Provide a value for 'region' : > eu-de Provide a value for 'worker_count' : The number of worker nodes that should be provisioned for classic infrastructure > ( 2 ) Provide a value for 'ibm-ocp-vpc_flavor' : The machine type that will be provisioned for classic infrastructure > ( bx2.4x16 ) Provide a value for 'ibm-vpc-subnets__count' : The number of subnets that should be provisioned > ( 1 ) Provide a value for 'resource_group_name' : The name of the resource group > default Output: Move on with the setup and apply Terraform. Do you want to perform these actions? Terraform will perform the actions described above. Only 'yes' will be accepted to approve. Enter a value: yes After a while you should get following output. Apply complete! Resources: 91 added, 0 changed, 0 destroyed. Major resources which were created: Cloud infrastructure/services resources 1 x VPC 1 x Subnet 4 Security groups Two were created during the subnet creation and two are related to the created Red Hat OpenShift cluster. 1 x Virtual Private Endpoint 1 x Public Gateway 2 x Access Control Lists One was created for the VPC module and one during the creation by the subnet module. 1 x Routing Table 1 x Red Hat OpenShift Cluster 1 x Object Storage Cluster and GitOps configuration Red Hat OpenShift GitOps operator and Red Hat OpenShift Pipelines operator GitHub project as ArgoCD repository Preconfigure ArgoCD project The invoked apply.sh script will create following files or folders: Inside the tools container: a temporary workspace/my-ibm-vpc-roks-argocd/variables.yaml.tmp file a workspace/my-ibm-vpc-roks-argocd/variables.yaml file a workspace/my-ibm-vpc-roks-argocd/terraform/variables.tf file a workspace/my-ibm-vpc-roks-argocd/terraform/variables.tfvars file several folders .kube , .terraform , .tmp , bin2 , docs Then it creates a terraform.tfvars file based on the entries you gave and executes init and apply command from Terraform. Be aware that the IBM Cloud access key information and GitHub access token are saved in text format in the output/my-ibm-vpc-roks-argocd/terraform/terraform.tfvars file! Don't share this in a public GitHub repository. On GitHub it creates a GitHub private project which contains preconfigure ArgoCD resource provided by cloud native toolkit Note: Here you can sample of the content of an example for a generated variables.yaml file link and here you can find an example for the created BOM file .","title":"Step 1 (inside the container): Execute the apply.sh and backup the result into the mapped volume"},{"location":"getting-started/lab3/#7-destroy-the-environment-on-ibm-cloud","text":"","title":"7. Destroy the environment on IBM Cloud"},{"location":"getting-started/lab3/#step-1-inside-the-container-destroy-the-created-ibm-cloud-resources","text":"All these tasks are automated in the helper bash script I wrote. Note: Ensure you didn't delete created Terraform files before. sh helper-tools-execute-destroy-and-delete-backup.sh * The script helper-tools-execute-destroy-and-delete-backup.sh does following: Navigate to workspace Execute destroy.sh Navigate to the mapped volume Copy the current state to the mapped volume Output: Note: It also deleted the automated created private GitHub project. Destroy complete! Resources: 91 destroyed.","title":"Step 1 (inside the container): Destroy the created IBM Cloud resources"},{"location":"getting-started/lab3/#8-summary","text":"We achieved what we wanted to achieve, create a customized initial setup in an IBM Cloud environment for GitOps . The Software Everywhere project and IasCable are powerful. As we have seen there was no need to write any Terraform module! Yes, when you are going to define you own \"Bill of Material BOM file\" you need to get familiar with the related modules related to your target architecture, when you want to customize it to your needs. But, as I said: There was no need to write own Terraform modules in our case. The Software Everywhere project and IasCable project needs some more documentation in the future, I like the power of it and it is under Apache-2.0 license , which means you can use it as your starting point for Software Everywhere with Terraform and contribute to the project.","title":"8. Summary"},{"location":"getting-started/whats-new/","text":"What's new \u00b6","title":"What's new"},{"location":"getting-started/whats-new/#whats-new","text":"","title":"What's new"},{"location":"learn/","text":"Learn \u00b6","title":"Learn"},{"location":"learn/#learn","text":"","title":"Learn"},{"location":"resources/","text":"Resources \u00b6","title":"Overview"},{"location":"resources/#resources","text":"","title":"Resources"},{"location":"resources/ascent/","text":"ASCENT \u00b6","title":"ASCENT"},{"location":"resources/ascent/#ascent","text":"","title":"ASCENT"},{"location":"resources/automation-bundles/","text":"Automation bundles \u00b6","title":"Automation bundles"},{"location":"resources/automation-bundles/#automation-bundles","text":"","title":"Automation bundles"},{"location":"resources/automation-runtimes/","text":"Automation runtime environments \u00b6 Supported runtimes \u00b6 There are two supported runtimes where the automation is expected to be executed inside of: Docker Desktop (Container engine) Multipass (VM) The Terraform automation can be run from the local operating system, but it is recommended to use either of the runtimes listed above, which provide a consistent and controlled environment, with all dependencies preinstalled. Docker Desktop \u00b6 Docker Desktop is an easy-to-use application that enables you to build and share containerized applications. It provides a simple interface that enables you to manage your containers, applications, and images directly from your machine without having to use the CLI to perform core actions. Docker Desktop is supported across Mac, Windows, and Linux, and can be downloaded and installed directly from: https://www.docker.com/products/docker-desktop/ Once installed, use the automation template's launch.sh script to launch an interactive shell where the Terraform automation can be executed. Multipass \u00b6 Multipass is a simplified Ubuntu Linux Virtual Machine that you can spin up with a single command. With this option you spin up a virtual machine with a predefined configuration that is ready to run the Terraform automation. You can download and install Multipass from https://multipass.run/install Once you have installed Multipass, open up a command line terminal and cd into the parent directory where you cloned the automation repo. Download the cloud-init script for use by the virtual machine using: curl https://raw.githubusercontent.com/cloud-native-toolkit/sre-utilities/main/cloud-init/cli-tools.yaml --output cli-tools.yaml The cli-tools cloud init script prepares a VM with the same tools available in the quay.io/cloudnativetoolkit/cli-tools-ibmcloud container image. Particularly: terraform terragrunt git jq yq oc kubectl helm ibmcloud cli Launch the Multipass virtual machine using the following command: multipass launch --name cli-tools --cloud-init ./cli-tools.yaml This will take several minutes to start the virtual machine and apply the configuration. Once the virtual machine is started, you need to mount the local file system for use within the virtual machine. Then mount the file system using the following command: multipass mount $PWD cli-tools:/automation This will mount the parent directory to the /automation directory inside of the virtual machine. \u26a0\ufe0f MacOS users may encounter the following error if Multipass has not been granted file system access. mount failed: source \"{current directory}\" is not readable If you encounter this error, then you need to enable full disk access in the operating system before you can successfully mount the volume. Go to System Preferences , then go to Security and Privacy , and select the Privacy tab. Scroll the list on the left and select \"Full Disk Access\" and allow access for multipassd . After granting access to multipassd , then re-run the multipass mount $PWD cli-tools:/automation command. Once the virtual machine has started, run the following command to enter an interactive shell: multipass shell cli-tools Once in the shell, cd into the /automation/{template} folder, where {template} is the Terraform template you configured. Then you need to load credentials into environment variables using the following command: source credentials.properties Once complete, you will be in an interactive shell that is pre-configured with all dependencies necessary to execute the Terraform automation. Unsupported runtimes \u00b6 Additional container engines, such as podman or colima may be used at your own risk. They may work, however, there are known issues using these environments, and the development team does not provide support for these environments. Known issues include: Network/DNS failures under load Read/write permissions to local storage volumes Issues running binary executables from volumes mounted from the host Time drift issues when hosts are suspended/resumed Colima instructions \u00b6 Install Brew Install Colima (a replacement for Docker Desktop ) and the docker cli brew install colima docker More information available at: https://github.com/abiosoft/colima#installation Podman instructions \u00b6 Unlike Docker which traditionally has separated a cli from a daemon-based container engine, Podman is a daemon-less container engine originally developed for Linux systems. There is a MacOS port which has sufficient features to support running the automation based on container images. Podman can run containers in root or rootless mode. Current permissions setup in the launch.sh script will require root mode. Getting started with Podman for MacOS \u00b6 Install Brew Install Podman (a replacement for Docker Desktop ) and the docker cli brew install podman docker Create a podman machine, set it to run in rootful mode and start it podman machine init podman machine set --rootful podman machine start Once the podman vm is started, use the automation template's launch.sh script to launch an interactive shell where the Terraform automation can be executed. Dealing with known issues for Podman on MacOS \u00b6 When resuming from suspend, if the podman machine is left running, it will not automatically synchronize to the host clock. This will cause the podman machine to lose time. Either stop/restart the podman machine or define an alias like this in your startup scripts: alias fpt = \"podman machine ssh \\\"sudo chronyc -m 'burst 4/4' makestep; date -u\\\"\" then fix podman time with the fpt command. There is currently an QEMU bug which prevents binary files that should be executable by the podman machine vm from operating from inside a mounted volume path. This is most common when using the host automation directory, vs a container volume like /workspaces for running the automation. Generally the cli-tools image will have any binary needed and the utils-cli module will symbolically link, vs. download a new binary into this path. However there can be drift between binaries in cli-tools image used by launch.sh and those requested to the utils-cli module. Getting started with Podman for Linux \u00b6 Visit and follow the installation instructions for your distribution Once the podman application is installed provide sudo podman as the first argument to the automation template's launch.sh script to launch an interactive shell where the Terraform automation can be executed: ./launch.sh 'sudo podman' More information available at: https://podman.io/getting-started/installation","title":"Automation Runtimes"},{"location":"resources/automation-runtimes/#automation-runtime-environments","text":"","title":"Automation runtime environments"},{"location":"resources/automation-runtimes/#supported-runtimes","text":"There are two supported runtimes where the automation is expected to be executed inside of: Docker Desktop (Container engine) Multipass (VM) The Terraform automation can be run from the local operating system, but it is recommended to use either of the runtimes listed above, which provide a consistent and controlled environment, with all dependencies preinstalled.","title":"Supported runtimes"},{"location":"resources/automation-runtimes/#docker-desktop","text":"Docker Desktop is an easy-to-use application that enables you to build and share containerized applications. It provides a simple interface that enables you to manage your containers, applications, and images directly from your machine without having to use the CLI to perform core actions. Docker Desktop is supported across Mac, Windows, and Linux, and can be downloaded and installed directly from: https://www.docker.com/products/docker-desktop/ Once installed, use the automation template's launch.sh script to launch an interactive shell where the Terraform automation can be executed.","title":"Docker Desktop"},{"location":"resources/automation-runtimes/#multipass","text":"Multipass is a simplified Ubuntu Linux Virtual Machine that you can spin up with a single command. With this option you spin up a virtual machine with a predefined configuration that is ready to run the Terraform automation. You can download and install Multipass from https://multipass.run/install Once you have installed Multipass, open up a command line terminal and cd into the parent directory where you cloned the automation repo. Download the cloud-init script for use by the virtual machine using: curl https://raw.githubusercontent.com/cloud-native-toolkit/sre-utilities/main/cloud-init/cli-tools.yaml --output cli-tools.yaml The cli-tools cloud init script prepares a VM with the same tools available in the quay.io/cloudnativetoolkit/cli-tools-ibmcloud container image. Particularly: terraform terragrunt git jq yq oc kubectl helm ibmcloud cli Launch the Multipass virtual machine using the following command: multipass launch --name cli-tools --cloud-init ./cli-tools.yaml This will take several minutes to start the virtual machine and apply the configuration. Once the virtual machine is started, you need to mount the local file system for use within the virtual machine. Then mount the file system using the following command: multipass mount $PWD cli-tools:/automation This will mount the parent directory to the /automation directory inside of the virtual machine. \u26a0\ufe0f MacOS users may encounter the following error if Multipass has not been granted file system access. mount failed: source \"{current directory}\" is not readable If you encounter this error, then you need to enable full disk access in the operating system before you can successfully mount the volume. Go to System Preferences , then go to Security and Privacy , and select the Privacy tab. Scroll the list on the left and select \"Full Disk Access\" and allow access for multipassd . After granting access to multipassd , then re-run the multipass mount $PWD cli-tools:/automation command. Once the virtual machine has started, run the following command to enter an interactive shell: multipass shell cli-tools Once in the shell, cd into the /automation/{template} folder, where {template} is the Terraform template you configured. Then you need to load credentials into environment variables using the following command: source credentials.properties Once complete, you will be in an interactive shell that is pre-configured with all dependencies necessary to execute the Terraform automation.","title":"Multipass"},{"location":"resources/automation-runtimes/#unsupported-runtimes","text":"Additional container engines, such as podman or colima may be used at your own risk. They may work, however, there are known issues using these environments, and the development team does not provide support for these environments. Known issues include: Network/DNS failures under load Read/write permissions to local storage volumes Issues running binary executables from volumes mounted from the host Time drift issues when hosts are suspended/resumed","title":"Unsupported runtimes"},{"location":"resources/automation-runtimes/#colima-instructions","text":"Install Brew Install Colima (a replacement for Docker Desktop ) and the docker cli brew install colima docker More information available at: https://github.com/abiosoft/colima#installation","title":"Colima instructions"},{"location":"resources/automation-runtimes/#podman-instructions","text":"Unlike Docker which traditionally has separated a cli from a daemon-based container engine, Podman is a daemon-less container engine originally developed for Linux systems. There is a MacOS port which has sufficient features to support running the automation based on container images. Podman can run containers in root or rootless mode. Current permissions setup in the launch.sh script will require root mode.","title":"Podman instructions"},{"location":"resources/module-catalog/","text":"Module catalog \u00b6","title":"Module catalog"},{"location":"resources/module-catalog/#module-catalog","text":"","title":"Module catalog"},{"location":"resources/troubleshooting/","text":"Troubleshooting \u00b6 Uninstalling \u00b6 To uninstall this solution: Use the ./launch.sh script to enter the docker container that was used to install this software package as described in README.md Navigate to the /workspaces/current folder: cd /workspaces/current There are 2 ways you can uninstall this solution: Use the /destroy-all.sh script to uninstall all layers of the solution Navigate into the specific subdirectories of the solution and remove specific layers. These steps should be applied for all layers, in reverse order, starting with the highest-numbered layer first. Repeat for all layers/sub-folders in your solution. cd 200-openshift-gitops terraform init terraform destroy --auto-approve Variables may not be used here. \u00b6 You may encounter an error message containing Variables may not be used here. during terraform execution, similar to the following: \u2502 Error: Variables not allowed \u2502 \u2502 on terraform.tfvars line 1: \u2502 1: cluster_login_token=asdf \u2502 \u2502 Variables may not be used here. This error happens when values in a tfvars file are not wrapped in quotes. In this case terraform interprets the value as a variable reference, which does not exist. To remedy this situation, wrap the value in your terraform.tfvars in quotes. For example: - cluster_login_token=ABCXYZ is incorrect - cluster_login_token=\"ABCXYZ\" is correct Intermittent network failures when using Colima \u00b6 If you are using the colima container engine (replacement for Docker Desktop), you may see random network failures when the container is put under heavy network load. This happens when the internal DNS resolver can't keep up with the container's network requests. The workaround is to switch colima to use external DNS instead of it's own internal DNS. Steps to fix this solution: Stop Colima using colima stop Create a file ~/.lima/_config/override.yaml containing the following: useHostResolver: false dns: - 8.8.8.8 Restart Colima using colima start Resume your activities where you encountered networking failures. It may be required to execute a terraform destroy command to cleanup invalid/bad state due to network failures. Resources stuck in Terminating state \u00b6 When deleting resources, the namespaces used by the solution occasionally will get stuck in a terminating or inconsistent state. Use the following steps to recover from these conditions: Follow these steps: - run oc get namespace <namespace> -o yaml on the CLI to get the details for the namespace. Within the yaml output, you can see if resources are stuck in a finalizing state. - Get the details of the remaining resource oc get <type> <instance> -n <namespace> -o yaml to see details on the resources that are stuck and have not been cleaned up. The <type> and <instance> can be found in the output of the previous oc get namespace <namespace> -o yaml command. - Patch the instances to remove the stuck finalizer: oc patch <type> <instance> -n <namespace> -p '{\"metadata\": {\"finalizers\": []}}' --type merge - Delete the resource that was stuck: oc delete <type> <instance> -n <namespace> - Go into ArgoCD instance and delete the remaining argo applications Workspace permission issues \u00b6 Root user on Linux \u00b6 If you are running on a linux machine as root user, the terraform directory is locked down so that only root had write permissions. When the launch.sh script puts you into the docker container, you are no longer root, and you encounter permission denied errors when executing setupWorkspace.sh . If the user on the host operating system is root , then you have to run chmod g+w -R . before running launch.sh to allow the terraform directory to be group writeable. Once you do this, the permission errors go away, and you can follow the installation instructions. Legacy launch.sh script \u00b6 IF you are not encountering the root user issue described above, and You may encounter permission errors if you have previously executed this terraform automation using an older launch.sh script (prior to June 2022). If you had previously executed the older launch.sh script, it mounted the workspace volume with root as the owner. The current launch.sh script mounts the workspace volume as the user devops . When trying to execute commands, you will encounter permission errors, and terraform or setupWorkspace.sh commands will only work if you use the sudo command. If this is the case, the workaround is to remove the workspace volume on your system, so that it can be recreated with the proper ownership. To do this: Exit the container using the exit command Verify that you have the workspace volume by executing docker volume list Delete the workspace volume using docker volume rm workspace If this command fails, you may first have to remove containers that reference the volume. User docker ps to list containers and docker rm <container> to remove a container. After you delete the container, re-run docker volume rm workspace to delete the workspace volume. Use the launch.sh script reenter the container. Use the setupWorkspace.sh script as described in the README.md to reconfigure your workspace and continue with the installation process. You should never use the sudo command to execute this automation. If you have to use sudo , then something is wrong with your configuration. That didn't work, what next? \u00b6 If you continue to experience issues with this automation, please file an issue or reach out on our public Discord server .","title":"Troubleshooting"},{"location":"resources/troubleshooting/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"resources/troubleshooting/#uninstalling","text":"To uninstall this solution: Use the ./launch.sh script to enter the docker container that was used to install this software package as described in README.md Navigate to the /workspaces/current folder: cd /workspaces/current There are 2 ways you can uninstall this solution: Use the /destroy-all.sh script to uninstall all layers of the solution Navigate into the specific subdirectories of the solution and remove specific layers. These steps should be applied for all layers, in reverse order, starting with the highest-numbered layer first. Repeat for all layers/sub-folders in your solution. cd 200-openshift-gitops terraform init terraform destroy --auto-approve","title":"Uninstalling"},{"location":"resources/troubleshooting/#variables-may-not-be-used-here","text":"You may encounter an error message containing Variables may not be used here. during terraform execution, similar to the following: \u2502 Error: Variables not allowed \u2502 \u2502 on terraform.tfvars line 1: \u2502 1: cluster_login_token=asdf \u2502 \u2502 Variables may not be used here. This error happens when values in a tfvars file are not wrapped in quotes. In this case terraform interprets the value as a variable reference, which does not exist. To remedy this situation, wrap the value in your terraform.tfvars in quotes. For example: - cluster_login_token=ABCXYZ is incorrect - cluster_login_token=\"ABCXYZ\" is correct","title":"Variables may not be used here."},{"location":"resources/troubleshooting/#intermittent-network-failures-when-using-colima","text":"If you are using the colima container engine (replacement for Docker Desktop), you may see random network failures when the container is put under heavy network load. This happens when the internal DNS resolver can't keep up with the container's network requests. The workaround is to switch colima to use external DNS instead of it's own internal DNS. Steps to fix this solution: Stop Colima using colima stop Create a file ~/.lima/_config/override.yaml containing the following: useHostResolver: false dns: - 8.8.8.8 Restart Colima using colima start Resume your activities where you encountered networking failures. It may be required to execute a terraform destroy command to cleanup invalid/bad state due to network failures.","title":"Intermittent network failures when using Colima"},{"location":"resources/troubleshooting/#resources-stuck-in-terminating-state","text":"When deleting resources, the namespaces used by the solution occasionally will get stuck in a terminating or inconsistent state. Use the following steps to recover from these conditions: Follow these steps: - run oc get namespace <namespace> -o yaml on the CLI to get the details for the namespace. Within the yaml output, you can see if resources are stuck in a finalizing state. - Get the details of the remaining resource oc get <type> <instance> -n <namespace> -o yaml to see details on the resources that are stuck and have not been cleaned up. The <type> and <instance> can be found in the output of the previous oc get namespace <namespace> -o yaml command. - Patch the instances to remove the stuck finalizer: oc patch <type> <instance> -n <namespace> -p '{\"metadata\": {\"finalizers\": []}}' --type merge - Delete the resource that was stuck: oc delete <type> <instance> -n <namespace> - Go into ArgoCD instance and delete the remaining argo applications","title":"Resources stuck in Terminating state"},{"location":"resources/troubleshooting/#workspace-permission-issues","text":"","title":"Workspace permission issues"},{"location":"resources/troubleshooting/#root-user-on-linux","text":"If you are running on a linux machine as root user, the terraform directory is locked down so that only root had write permissions. When the launch.sh script puts you into the docker container, you are no longer root, and you encounter permission denied errors when executing setupWorkspace.sh . If the user on the host operating system is root , then you have to run chmod g+w -R . before running launch.sh to allow the terraform directory to be group writeable. Once you do this, the permission errors go away, and you can follow the installation instructions.","title":"Root user on Linux"},{"location":"resources/troubleshooting/#legacy-launchsh-script","text":"IF you are not encountering the root user issue described above, and You may encounter permission errors if you have previously executed this terraform automation using an older launch.sh script (prior to June 2022). If you had previously executed the older launch.sh script, it mounted the workspace volume with root as the owner. The current launch.sh script mounts the workspace volume as the user devops . When trying to execute commands, you will encounter permission errors, and terraform or setupWorkspace.sh commands will only work if you use the sudo command. If this is the case, the workaround is to remove the workspace volume on your system, so that it can be recreated with the proper ownership. To do this: Exit the container using the exit command Verify that you have the workspace volume by executing docker volume list Delete the workspace volume using docker volume rm workspace If this command fails, you may first have to remove containers that reference the volume. User docker ps to list containers and docker rm <container> to remove a container. After you delete the container, re-run docker volume rm workspace to delete the workspace volume. Use the launch.sh script reenter the container. Use the setupWorkspace.sh script as described in the README.md to reconfigure your workspace and continue with the installation process. You should never use the sudo command to execute this automation. If you have to use sudo , then something is wrong with your configuration.","title":"Legacy launch.sh script"},{"location":"resources/troubleshooting/#that-didnt-work-what-next","text":"If you continue to experience issues with this automation, please file an issue or reach out on our public Discord server .","title":"That didn't work, what next?"}]}